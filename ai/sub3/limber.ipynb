{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQjM0bpnL4oa"
      },
      "source": [
        "## 과제 목표\n",
        "1. LiMBeR 모델 Architecture의 정성적 이해\n",
        "2. 학습된 CLIP과 LLM을 활용하여 대규모 시각-언어 멀티모달 LLM을 구축\n",
        "3. 시각-언어 멀티모달 LLM 학습\n",
        "3. 학습된 시각-언어 멀티모달 LLM을 활용한 Image Captioning Task\n",
        "\n",
        "## 참고 자료\n",
        " - Paper : [LiMBeR](https://arxiv.org/abs/2209.15162)\n",
        " - Code : [LiMBeR github](https://github.com/jmerullo/limber), [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\n",
        " - Dataset : [CC3M](https://github.com/google-research-datasets/conceptual-captions)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgsWhnkiSlq7"
      },
      "source": [
        "## 0. 환경 세팅\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "함께 포함된 limber.zip 파일을 압축 해제하여 해당 limber 폴더를 sub3 폴더 안에 넣어줍니다.  \n",
        "\n",
        "디렉토리 구조는 다음과 같습니다.\n",
        "- 디렉토리 구조  \n",
        "      |-- project/  \n",
        "      |   |-- sub1/  \n",
        "      |   |-- sub2/   \n",
        "      |   |-- sub3/  \n",
        "      |   |   |-- limber/  \n",
        "      |   |   |-- limber.ipynb   \n",
        "      |   |   |-- vqa.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDmsHw7WIx0Q"
      },
      "source": [
        "Colab에서 진행하는 경우, 아래 코드 주석을 해제하고 셀을 실행시켜 구글 드라이브 마운트를 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGCTryKHTFul"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEi6N2rbIx0R"
      },
      "source": [
        "Colab에서 진행하는 경우, 구글 드라이브에 project 디렉토리를 업로드합니다.   \n",
        "이후, 아래 코드 주석을 해제하고 셀을 실행시켜 현재 작업중인 디렉토리를 이동합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEnOuTwfTGeq"
      },
      "outputs": [],
      "source": [
        "# cd \"/content/drive/MyDrive/project/sub3/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpK-2bX7T2hw"
      },
      "source": [
        "필요한 pakage들을 설치합니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkqEWt_z0OHS"
      },
      "source": [
        "      pip install transformers\n",
        "      pip install https://github.com/openai/CLIP/archive/master.zip\n",
        "      pip install torchtyping\n",
        "      pip install einops\n",
        "      pip install pyhelpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuoE8wOW1rh9"
      },
      "source": [
        "Colab에서 진행하는 경우, 아래 코드 주석을 해제하고 셀을 실행시켜 필요한 pakage들을 설치할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgr9rxvS-UtQ"
      },
      "outputs": [],
      "source": [
        "# !pip install -qq transformers\n",
        "# !pip install -qq https://github.com/openai/CLIP/archive/master.zip\n",
        "# !pip install -qq torchtyping\n",
        "# !pip install -qq einops\n",
        "# !pip install -qq pyhelpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh0KAG8h-UtQ"
      },
      "source": [
        "seed를 고정해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTyd92L-UtR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random_seed = 1111\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG5LdNxdCVoG"
      },
      "source": [
        "## 1. Image Captioning Data 전처리  \n",
        "\n",
        "**1-1. CC3M(Conceptual Captions) Dataset 설명**    \n",
        "\n",
        "이번 실습에서는 [CC3M](https://github.com/google-research-datasets/conceptual-captions) 데이터셋을 훈련 및 추론에 사용할 것입니다. CC3M은 이미지 캡셔닝 시스템의 훈련 및 평가를 위해 설계된 300만 개 이상의 (이미지 URL, 캡션) 쌍을 포함하는 데이터셋입니다.\n",
        "\n",
        "다음은 CC3M 데이터셋의 예시입니다.\n",
        "\n",
        "이미지:  \n",
        "\n",
        "![image.png](https://thumb7.shutterstock.com/display_pic_with_logo/1350382/147082805/stock-vector-white-swan-on-the-blue-background-147082805.jpg)  \n",
        "(그림 출처: https://thumb7.shutterstock.com/display_pic_with_logo/1350382/147082805/stock-vector-white-swan-on-the-blue-background-147082805.jpg)\n",
        "\n",
        "캡션:\n",
        "white swan on the blue background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwCX5ixLIVmr"
      },
      "source": [
        "이번 실습은 저희가 제공하는 소량의 CC3M 데이터셋을 사용합니다. 더 많은 데이터로 실습을 진행하고자 하는 분들은 아래 1-2 지침을 따라 데이터셋을 다운로드하고 디렉토리 구조를 만든 후 진행하실 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mOWOzNDLSnk"
      },
      "source": [
        "**1-2. CC3M 데이터셋 다운로드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiMOl5pTt8-q"
      },
      "source": [
        "- 다운로드 링크 : https://huggingface.co/datasets/zatepyakin/cc3m_min256_max512/tree/main\n",
        "\n",
        "- 다운로드 예시\n",
        "\n",
        "      mkdir cc3m && cd cc3m\n",
        "      mkdir training && cd training\n",
        "\n",
        "      wget https://huggingface.co/datasets/zatepyakin/cc3m_min256_max512/resolve/main/00000.tar\n",
        "      tar -xvf \"00000.tar\"\n",
        "\n",
        "      mkdir images\n",
        "      mv *.jpg images\n",
        "\n",
        "      mkdir image_data\n",
        "      mv *.json image_data\n",
        "\n",
        "      rm *.txt\n",
        "\n",
        "- 디렉토리 구조  \n",
        "      |-- cc3m  \n",
        "      |   |-- train  \n",
        "      |   |   |-- image_data  \n",
        "      |   |   |   |-- 000000000.json    \n",
        "      |   |   |   |-- 000000001.json   \n",
        "      |   |   |   |-- ...  \n",
        "      |   |   |-- images  \n",
        "      |   |   |   |-- 000000000.jpg    \n",
        "      |   |   |   |-- 000000001.jpg   \n",
        "      |   |   |   |-- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEWJvjK0FHbg"
      },
      "source": [
        "**1-3. Tokenizer 정의**  \n",
        "텍스트 데이터를 Language Model의 입력으로 사용하기 위해서는 토큰 형태로 변환해야 합니다. 🤗 Transformers 라이브러리는 이를 위한 일련의 규칙에 따라 텍스트를 토큰 시퀀스로 변환하는 토크나이저를 제공합니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxKZYqnG1hV"
      },
      "source": [
        "Transformers 라이브러리에서 제공하는 tokenizer를 사용하여 tokenizer를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUVIqZKnGSo6"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "# transformers 라이브러리에서 제공하는 pretrained tokenizer를 로드합니다.\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "# pad token id를 eos token id로 지정합니다.\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# 모델이 padding을 오른쪽에 적용하도록 합니다.\n",
        "tokenizer.padding_side = \"right\"\n",
        "# 모델의 입력에 대한 최대 길이(토큰 수로 측정)를 지정합니다.\n",
        "tokenizer.model_max_length = 2048\n",
        "# '<|image|>' 문자열을 클래스 토큰으로 설정하고 tokenizer에 추가합니다.\n",
        "tokenizer.add_special_tokens(\n",
        "            {'cls_token': '<|image|>'}\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEjooBbVGSpG"
      },
      "source": [
        "정의한 tokenizer의 encode 함수를 사용하여 텍스트를 토크나이저에 전달해보겠습니다. 아래 결과를 보면, 일련의 규칙에 따라 텍스트가 토큰 시퀀스 인덱스로 변환된 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcvgmMviGSpG"
      },
      "outputs": [],
      "source": [
        "encoded_input = tokenizer.encode(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfGh6HpTGSpG"
      },
      "source": [
        "토큰 시퀀스 인덱스는 tokenizer의 decode 함수를 통해 원래의 text로 반환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s6RQoANGSpG"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(encoded_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cPXuwE4JV-n"
      },
      "source": [
        "원래의 text가 나오는 것을 확인할 수 있습니다.  \n",
        "이제 tokenizer에 저장되어있는 vocabulary를 불러와서 출력해보도록 하겠습니다.  \n",
        "표현 가능한 vocabulary는 우리가 위에서 tokenizer에 추가한 '<|image|>' 토큰을 포함하여 총 50258개 입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHPvil8oN9_R"
      },
      "outputs": [],
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "\n",
        "print(list(vocab.items())[:10])\n",
        "print(f\"len(vocab) : {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BVa_pTxQcIg"
      },
      "source": [
        "Vocab의 가장 마지막에 있는 토큰이 우리가 추가한 '<|image|>' 토큰임을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ_2Pci_P23X"
      },
      "outputs": [],
      "source": [
        "print(vocab['<|image|>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxjhVliAAciS"
      },
      "source": [
        "**1-4. ImageCaptionDataset class 정의**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ddhehEIhGx"
      },
      "source": [
        "CC3M 데이터와 tokenizer가 준비되었으니, 이제 데이터를 모델이 예상하는 입력 형식으로 전처리해주는 custom dataset class를 정의합니다.  \n",
        "\n",
        "ImageCaptionDataset 클래스를 구현하면서 Req. 1-1을 풀어봅니다. 구현에 필요한 부분은 다음과 같습니다. 해당 \"\"\"Write your code\"\"\" 부분을 구현합니다.\n",
        "\n",
        "- ImageCaptionDataset 클래스\n",
        "  - **Req. 1-1**: 데이터 전처리 : ImageCaptionDataset 내 \\_\\_getitem\\_\\_() 메소드 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09UQpKL8UCBK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torchtyping import TensorType\n",
        "from typing import Tuple\n",
        "import os\n",
        "from PIL import Image\n",
        "from pyhelpers.store import load_json\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, data_dir, tokenizer=None, transforms=None, seq_len=2048\n",
        "    ):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        paths = []\n",
        "        img_data_dir = self.data_dir / \"image_data\"\n",
        "        for p in tqdm(Path(img_data_dir).glob(f\"*.json\"), desc=f\"loading dataset paths from {str(img_data_dir)}\"):\n",
        "          paths.append(p)\n",
        "        self.paths = sorted(paths)\n",
        "        print(\"LENGTH OF DATA\", len(self.paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-1: 데이터 전처리 : ImageCaptionDataset 내 __getitem__() 메소드 구현       #\n",
        "    ############################################################################\n",
        "    def __getitem__(\n",
        "        self, idx\n",
        "    ) -> Tuple[TensorType[\"b\", \"c\", \"h\", \"w\"], TensorType[\"b\", \"s\"]]:\n",
        "\n",
        "        img_data = load_json(self.paths[idx])\n",
        "        img_path = os.path.join(self.data_dir, \"images\", img_data['key']+\".jpg\")\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: img_path를 텐서 형태의 이미지로, caption을 transformers library의 tokenizer를 #\n",
        "        # 통해 텐서 형태의 토큰으로 변경                                                      #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) PIL library의 Image.open() 함수를 사용해서 img_path로부터 RGB 형식으로 변환된\n",
        "        #    PIL.Image 이미지 객체 얻기\n",
        "        # 2) PIL.Image 이미지 객체를 사용자 정의된 self.transforms를 이용하여 전처리하여\n",
        "        #    [channel, height, width] 형태의 텐서로 변환\n",
        "        # 3) Transformers library의 tokenizer에서 정의된 encode() 함수로 caption을\n",
        "        #    텐서 형태의 token으로 변환\n",
        "\n",
        "        img = \"\"\"Write your code\"\"\"\n",
        "        img_tensor = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        caption = img_data[\"caption\"]\n",
        "        token_tensor = self.tokenizer.encode(\n",
        "            \"\"\"Write your code\"\"\",\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.seq_len,\n",
        "            padding=\"longest\",\n",
        "            truncation=True,\n",
        "        ).squeeze()\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return {'images':img_tensor, 'captions':token_tensor}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPQo_vU3PQ6I"
      },
      "source": [
        "# 2. LiMBeR 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqSEW96UKyq8"
      },
      "source": [
        "LiMBeR(Linearly Mapping Between Representation spaces) 모델의 기본 접근 방식은 사전 훈련된 이미지 인코더의 hidden size $h_I$를 언어 모델의 input space로 투영하기 위해 linear layer P를 훈련시키는 것입니다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jchK4RN8QrAY"
      },
      "source": [
        "**2-1. Image Encoder**  \n",
        "이미지 인코더로부터 이미지를 나타내는 $h_I$ 차원의 이미지 표현을 추출합니다.\n",
        "\n",
        "**2-2. Projection Layer**  \n",
        "Projection $P$는 이미지 표현을 $e_L$ * $k$ 시퀀스의 소프트 프롬프트로 투영합니다. 이를 이미지 프롬프트라고 지칭합니다.\n",
        "\n",
        "**2-3. Text Decoder (Large Language Model)**  \n",
        "이미지 프롬프트를 사용하여 언어 모델을 prompting함으로써, 언어 모델은 이미지를 설명하는 캡션을 생성할 수 있습니다.    \n",
        "\n",
        "Projection Layer의 양쪽에 있는 이미지 인코더와 언어 모델을 동결하여 학습된 파라미터가 프로젝션 레이어에만 존재하도록 합니다.\n",
        "\n",
        "본 실습에서는 이미지 인코더로 **CLIP RN50x16**를 사용하며 **$k$=144, $h_I$=3072, $e_L$=2048** 로 설정합니다.  \n",
        "또한 논문에서는 언어 모델로 60억 개의 파라미터를 가진 decoder-only GPT-J 모델을 사용하였으나, Colab의 자원 제한으로 인해 본 실습에서는 13억 개의 파라미터를 가진 **decoder-only GPT-Neo** 모델을 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY-vWh_kLcZj"
      },
      "source": [
        "ImagePrefix 클래스와 LimberGPTJ 클래스의 함수들을 하나씩 구현하면서 Req. 1-2 부터 Req. 1-5를 풀어봅니다. 구현에 필요한 부분은 다음과 같습니다.\n",
        "- ImagePrefix 클래스  \n",
        "  - **Req. 1-2**: 이미지를 입력 받아 소프트 프롬프트를 반환하는 ImagePrefix 클래스 구현\n",
        "- LimberGPTJ 클래스  \n",
        "  - **Req. 1-3**: LimberGPTJ 내 setmultimodal 함수 구현\n",
        "  - **Req. 1-4**: LimberGPTJ 내 make_input_embeddings 함수 구현\n",
        "  - **Req. 1-5**: LimberGPTJ 내 build_labels_for_training 함수 구현\n",
        "\n",
        "아래 셀부터 하나씩 따라가면서 Req. 1-2 부터 Req. 1-5 에서 구현이 필요한 부분을 읽고, 다시 ImagePrefix 클래스 또는 LimberGPTJ 클래스로 돌아와서 해당 \"\"\"Write your code\"\"\" 부분을 구현해봅니다.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6lHXEqHHIWM"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Union\n",
        "import torch.nn as nn\n",
        "import clip\n",
        "from einops import rearrange\n",
        "\n",
        "############################################################################\n",
        "# Req 1-2: 이미지를 입력 받아 소프트 프롬프트를 반환하는 ImagePrefix 클래스 구현         #\n",
        "############################################################################\n",
        "class ImagePrefix(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    이미지 배치를 입력으로 받아 언어 모델의 word embedding과 동일한 h_I 차원의 소프트 프롬프트를 반환합니다.\n",
        "\n",
        "    :param encoder_out_dim: 이미지 인코더의 출력 차원 (=h_I)\n",
        "    :param lm_out_dim: 언어 모델의 입력 및 출력 차원 (=e_L)\n",
        "    :param out_seq_len: 소프트 프롬프트의 시퀀스 길이 (=k)\n",
        "    :param image_embed_dropout_prob: 소프트 프롬프트에 적용할 dropout probability\n",
        "    :param device: 모델을 실행 할 장치\n",
        "    \"\"\"\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO: 텐서 형태 이미지를 입력받아 CLIP의 image encoder를 통해 이미지 표현을 얻고          #\n",
        "    # projection layer를 통해 소프트 프롬프트로 투영하는 ImagePrefix 클래스 구현              #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) __init__() 함수에서 pretrained CLIP image encoder 가져오기\n",
        "    # 2) __init__() 함수에서 projection layer 정의\n",
        "    # 3) __forward__() 함수에서 pretrained CLIP image encoder를 사용하여 image로부터 이미지 표현 얻기\n",
        "    # 4) __forward__() 함수에서 projection layer를 사용하여 이미지 표현을 soft prompts로 투영\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, device=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # pretrained CLIP image encoder 가져오기\n",
        "        self.encoder = \"\"\"Write your code\"\"\"\n",
        "        self.encoder.attnpool = Lambda(\n",
        "            partial(rearrange, pattern=\"b d h w -> b (h w) d\")\n",
        "        )\n",
        "        # 이미지 인코더의 출력 차원 (=h_I)\n",
        "        self.encoder_out_dim = encoder_out_dim\n",
        "\n",
        "        # 언어 모델의 입력 및 출력 차원 (=e_L)\n",
        "        self.lm_out_dim = lm_out_dim\n",
        "\n",
        "        # 소프트 프롬프트의 시퀀스 길이 (=k)\n",
        "        self.out_seq_len = out_seq_len\n",
        "\n",
        "        # 소프트 프롬프트의 차원\n",
        "        proj_out_dim = self.lm_out_dim\n",
        "\n",
        "        # Projection layer 정의\n",
        "        self.proj = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        self.dropout = nn.Dropout(image_embed_dropout_prob)\n",
        "\n",
        "    def forward(\n",
        "        self, x: TensorType[\"b\", \"c\", \"h\", \"w\"]\n",
        "    ) -> TensorType[\"b\", \"seq\", \"out_dim\"]:\n",
        "\n",
        "        image_feats = \"\"\"Write your code\"\"\"\n",
        "        soft_prompts = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # 소프트 프롬프트에 dropout 적용\n",
        "        soft_prompts = self.dropout(soft_prompts)\n",
        "\n",
        "        return soft_prompts\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sahzR_qvP6lt"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM\n",
        "from typing import List, Optional\n",
        "from torchtyping import TensorType\n",
        "from transformers.file_utils import ModelOutput\n",
        "from torchvision import transforms as T\n",
        "\n",
        "class LimberGPTNeo(GPTNeoForCausalLM):\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-3: LimberGPTJ 내 setmultimodal 함수 구현                              #\n",
        "    ############################################################################\n",
        "    def setup_multimodal(self, tokenizer, max_seq_len, encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, freeze_lm, freeze_img_encoder):\n",
        "\n",
        "        \"\"\"\n",
        "        :param tokenizer: tokenizer\n",
        "        :param max_seq_len: 언어 모델의 입력에 대한 최대 길이(토큰 수로 측정)\n",
        "        :param encoder_out_dim: 이미지 인코더의 출력 차원 (=h_I)\n",
        "        :param lm_out_dim: 언어 모델의 입력 및 출력 차원 (=e_L)\n",
        "        :param out_seq_len: 소프트 프롬프트의 시퀀스 길이 (=k)\n",
        "        :param image_embed_dropout_prob: 소프트 프롬프트에 적용할 dropout probability\n",
        "        :param freeze_lm: 언어 모델을 freeze할지 여부\n",
        "        :param freeze_img_encoder: 이미지 인코더를 freeze할지 여부\n",
        "        \"\"\"\n",
        "        ################################################################################\n",
        "        # TODO: Transformers의 GPTNeoForCausalLM 클래스를 상속받는 LimberGPTJ 클래스에서       #\n",
        "        # ImagePrefix 클래스의 인스턴스를 생성한다. Image encoder와 Language model은 freeze시켜  #\n",
        "        # Image Encoder(freeze) – Projection Layer – Language Model(freeze) 구조를 만든다.#\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) 언어 모델 freeze\n",
        "        # 2) ImagePrefix 클래스의 인스턴스 생성\n",
        "        # 3) 이미지 인코더 freeze\n",
        "\n",
        "        self.seq_len = max_seq_len\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.image_token = self.tokenizer.cls_token_id\n",
        "        self.eos_token = self.tokenizer.eos_token_id\n",
        "        self.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        self.word_embedding = self.transformer.wte\n",
        "\n",
        "        # 언어 모델 freeze\n",
        "        if freeze_lm:\n",
        "            \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # ImagePrefix 클래스의 인스턴스 생성\n",
        "        self.image_prefix = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # 이미지 프롬프트의 시퀀스 길이 지정\n",
        "        self.image_prompt_seq_len = self.image_prefix.out_seq_len\n",
        "        inp_rez = self.image_prefix.encoder.input_resolution\n",
        "\n",
        "        self.transforms = T.Compose(\n",
        "        [\n",
        "            T.Resize(inp_rez, interpolation=T.InterpolationMode.BICUBIC),\n",
        "            T.CenterCrop(inp_rez),\n",
        "            lambda image: image.convert(\"RGB\"),\n",
        "            T.ToTensor(),\n",
        "            add_batch_dim,\n",
        "            T.Normalize(\n",
        "                (0.48145466, 0.4578275, 0.40821073),\n",
        "                (0.26862954, 0.26130258, 0.27577711),\n",
        "            ),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "        # 이미지 인코더 freeze\n",
        "        if freeze_img_encoder:\n",
        "            \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-4: LimberGPTJ 내 make_input_embeddings 함수 구현                     #\n",
        "    ############################################################################\n",
        "    def make_input_embeddings(self,\n",
        "        images: TensorType[\"b\", \"c\", \"h\", \"w\"] = None,\n",
        "        captions: Optional[TensorType[\"b\", \"seq\"]] = None,\n",
        "        image_embeddings: TensorType[\"b\", \"s\", \"d\"] = None,):\n",
        "\n",
        "        \"\"\"\n",
        "        :param images: 이미지\n",
        "        :param captions: 캡션\n",
        "        :param image_embeddings: 이미지 프롬프트 (inference 시 사용)\n",
        "        \"\"\"\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: ImagePrefix 클래스의 인스턴스를 이용하여 입력 이미지로부터 이미지 프롬프트를 얻고,      #\n",
        "        # 캡션으로부터 캡션의 임베딩을 얻어 훈련 시 LLM에 인풋으로 들어갈 임베딩 얻기                   #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) 이미지 프롬프트 추출\n",
        "        # 2) 캡션 임베딩 추출\n",
        "        # 3) 훈련 시 LLM에 인풋으로 들어갈 임베딩 만들기\n",
        "\n",
        "        if image_embeddings is None:\n",
        "\n",
        "        \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return image_embeddings, caption_embeddings, input_embeddings\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-5: LimberGPTJ 내 build_labels_for_training 함수 구현                 #\n",
        "    ############################################################################\n",
        "    def build_labels_for_training(self,\n",
        "        image_embeddings: TensorType[\"b\", \"s\", \"d\"] = None,\n",
        "        captions: Optional[TensorType[\"b\", \"seq\"]] = None,):\n",
        "\n",
        "        \"\"\"\n",
        "        :param image_embeddings: 이미지 프롬프트\n",
        "        :param captions: 캡션\n",
        "        \"\"\"\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: 이미지와 캡션으로부터 언어 모델을 훈련하기 위한 라벨을 만든다.                        #\n",
        "        # 이미지 프롬프트와, text에서 첫 eos 토큰 이후 반복되는 eos 토큰에 대해서는 loss가            #\n",
        "        # 계산되지 않아야 하기 때문에 해당하는 label을 -100으로 설정해주어 masking 해준다.            #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) 이미지 프롬프트의 length 만큼의 label을 -100으로 설정한다.\n",
        "        # 2) 이미지 프롬프트의 label과 caption을 concat하여 언어 모델을 훈련하기 위한 라벨을 만든다.\n",
        "        # 3) text에서 첫 eos 토큰 이후 반복되는 eos 토큰에 대해서 label을 -100으로 설정한다.\n",
        "\n",
        "        \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return labels\n",
        "\n",
        "\n",
        "    def preprocess_inputs(self, img_path, text_prompt):\n",
        "\n",
        "        return preprocess_inputs(\n",
        "            self,\n",
        "            img_path=img_path,\n",
        "            text_prompt=text_prompt,\n",
        "        )\n",
        "\n",
        "    def top_k_sampling(self, logits, top_k, temperature):\n",
        "\n",
        "        return top_k_sampling(\n",
        "            self,\n",
        "            logits=logits,\n",
        "            top_k=top_k,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "    def top_p_sampling(self, logits, top_p, temperature):\n",
        "\n",
        "        return top_p_sampling(\n",
        "            self,\n",
        "            logits=logits,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        embeddings: TensorType[\"b\", \"s\", \"d\"],\n",
        "        max_steps: int = 100,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 0.9,\n",
        "        decode: bool = True,\n",
        "        temperature: float = 0.7,\n",
        "\n",
        "    ):\n",
        "\n",
        "        return generate(\n",
        "            self,\n",
        "            embeddings=embeddings,\n",
        "            max_steps=max_steps,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            decode=decode,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: TensorType[\"b\", \"c\", \"h\", \"w\"] = None,\n",
        "        captions: Optional[TensorType[\"b\", \"seq\"]] = None,\n",
        "        output_hidden_states: bool = True,\n",
        "        image_embeddings: TensorType[\"b\", \"s\", \"d\"] = None,\n",
        "        attention_mask=None,\n",
        "    ) -> ModelOutput:\n",
        "\n",
        "        # 훈련 시 LLM에 인풋으로 들어갈 임베딩을 얻는다.\n",
        "        image_embeddings, caption_embeddings, input_embeddings = self.make_input_embeddings(images,captions,image_embeddings)\n",
        "\n",
        "        # 이미지와 캡션으로부터 언어 모델을 훈련하기 위한 라벨을 만든다.\n",
        "        labels = self.build_labels_for_training(image_embeddings, captions)\n",
        "\n",
        "        # Language Model 추론\n",
        "        lm_outputs = super().forward(\n",
        "            inputs_embeds=input_embeddings,\n",
        "            labels=labels,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=False,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        return lm_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k82NzVNNSOjh"
      },
      "source": [
        "하이퍼파라미터를 설정해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "494mC_TkuEV6"
      },
      "outputs": [],
      "source": [
        "# Dataset parameters\n",
        "train_dataset_dir='./limber/cc3m/train'\n",
        "\n",
        "# Model parameters\n",
        "max_seq_len=2048 # 언어 모델의 입력에 대한 최대 길이(토큰 수로 측정)\n",
        "encoder_out_dim=3072 # 이미지 인코더의 출력 차원 (=h_I)\n",
        "lm_out_dim=2048 # 언어 모델의 입력 및 출력 차원 (=e_L)\n",
        "out_seq_len=144 # 이미지 프롬프트 시퀀스 길이 (=k)\n",
        "image_embed_dropout_prob=0.1 # 소프트 프롬프트에 적용할 dropout probability\n",
        "\n",
        "# Training parameters\n",
        "weight_decay=0.0\n",
        "train_batch_size=16\n",
        "min_lr=0.0\n",
        "lr=8.0e-4\n",
        "image_enc_lr=2e-06\n",
        "gradient_accumulation_steps=8\n",
        "train_steps=100\n",
        "freeze_lm=True\n",
        "freeze_img_encoder=True\n",
        "save='./limber/checkpoints'\n",
        "exp_name='train'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnUBBmr3Xy9-"
      },
      "source": [
        "학습 및 추론에 필요한 유틸리티 클래스와 유틸리티 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-51Hz3WFXt04"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "import  PIL.Image as PilImage\n",
        "from dataclasses import dataclass\n",
        "from typing import Union, Any, Callable\n",
        "from transformers.utils import PaddingStrategy\n",
        "from collections.abc import Mapping\n",
        "from collections import defaultdict\n",
        "\n",
        "# 유틸리티 클래스\n",
        "class Lambda(torch.nn.Module):\n",
        "    def __init__(self, fn: Callable):\n",
        "        super().__init__()\n",
        "        assert hasattr(fn, \"__call__\")\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(x)\n",
        "\n",
        "@dataclass\n",
        "class ImageCaptionDataCollator:\n",
        "    tokenizer: GPT2TokenizerFast\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = {}\n",
        "        batch['input_ids']=[f[\"captions\"] for f in features]\n",
        "        batch = self.tokenizer.pad(\n",
        "            batch,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=self.return_tensors,\n",
        "        )\n",
        "        batch['captions']=batch['input_ids']\n",
        "        batch['images'] = torch.stack([i['images'] for i in features]).float()\n",
        "        batch['attention_mask'] = torch.stack([torch.cat( (torch.ones(out_seq_len), a) ) for a in batch['attention_mask']]).float()\n",
        "        del batch['input_ids']\n",
        "        return batch\n",
        "\n",
        "# 유틸리티 함수\n",
        "def add_batch_dim(t):\n",
        "  return t.unsqueeze(0)\n",
        "\n",
        "def get_params_for_weight_decay_optimization(module):\n",
        "    weight_decay_params = {\"params\": []}\n",
        "    no_weight_decay_params = {\"params\": [], \"weight_decay\": 0.0}\n",
        "    blacklist_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "\n",
        "    for module_ in module.modules():\n",
        "        if isinstance(module_, blacklist_modules) or (\n",
        "            weight_decay == 0.0\n",
        "        ):\n",
        "            no_weight_decay_params[\"params\"].extend(\n",
        "                [\n",
        "                    p\n",
        "                    for p in list(module_._parameters.values())\n",
        "                    if (p is not None) and p.requires_grad\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            for n, p in list(module_._parameters.items()):\n",
        "                if p is not None and p.requires_grad:\n",
        "                    if n != \"bias\":\n",
        "                        weight_decay_params[\"params\"].append(p)\n",
        "                    else:\n",
        "                        no_weight_decay_params[\"params\"].append(p)\n",
        "    param_dict = {\n",
        "        pn: p\n",
        "        for pn, p in module.named_parameters()\n",
        "        if p is not None and p.requires_grad\n",
        "    }\n",
        "    assert len(no_weight_decay_params[\"params\"]) + len(\n",
        "        weight_decay_params[\"params\"]\n",
        "    ) == len(\n",
        "        param_dict.keys()\n",
        "    ), \"Number of params in both groups != total number of trainable params\"\n",
        "    if weight_decay == 0.0:\n",
        "        # only return a single param group if no weight decay is being used anyway\n",
        "        return [no_weight_decay_params]\n",
        "    return [weight_decay_params, no_weight_decay_params]\n",
        "\n",
        "def configure_param_groups(model):\n",
        "    if image_enc_lr is not None:\n",
        "        image_enc_params = get_params_for_weight_decay_optimization(\n",
        "            model.image_prefix.encoder\n",
        "        )\n",
        "        for pdict in image_enc_params:\n",
        "            pdict[\"lr\"] = image_enc_lr\n",
        "        image_proj_params = get_params_for_weight_decay_optimization(\n",
        "            model.image_prefix.proj\n",
        "        )\n",
        "\n",
        "        lm_params = get_params_for_weight_decay_optimization(model.transformer)\n",
        "        lm_params +=get_params_for_weight_decay_optimization(model.lm_head)\n",
        "\n",
        "        class_params = []\n",
        "        if hasattr(model, \"class_head\") and model.class_head is not None:\n",
        "            class_params = get_params_for_weight_decay_optimization(\n",
        "                model.class_head\n",
        "            )\n",
        "        all_params = []\n",
        "        for p in image_enc_params + lm_params + image_proj_params + class_params:\n",
        "            if p[\"params\"]:\n",
        "                all_params.append(p)\n",
        "    else:\n",
        "        all_params = get_params_for_weight_decay_optimization(model)\n",
        "\n",
        "    d = defaultdict(dict)\n",
        "    for param_group in all_params:\n",
        "        lr = param_group.get(\"lr\", None)\n",
        "        wd = param_group.get(\"weight_decay\", None)\n",
        "        key = f\"lr_{lr}_wd_{wd}\"\n",
        "        if d[key].get(\"params\") is None:\n",
        "            d[key][\"params\"] = []\n",
        "        d[key][\"params\"].extend(param_group[\"params\"])\n",
        "        if lr is not None:\n",
        "            d[key][\"lr\"] = lr\n",
        "        if wd is not None:\n",
        "            d[key][\"weight_decay\"] = wd\n",
        "    all_params = list(d.values())\n",
        "    n_params = sum([len(d[\"params\"]) for d in all_params])\n",
        "    param_dict = {\n",
        "        pn: p for pn, p in model.named_parameters() if p is not None and p.requires_grad\n",
        "    }\n",
        "    assert n_params == len(\n",
        "        param_dict\n",
        "    ), f\"Some parameters are missing from param groups ({n_params} | {len(param_dict)})\"\n",
        "    return all_params\n",
        "\n",
        "def _prepare_input(data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:\n",
        "    if isinstance(data, Mapping):\n",
        "        return type(data)({k: _prepare_input(v) for k, v in data.items()})\n",
        "    elif isinstance(data, (tuple, list)):\n",
        "        return type(data)(_prepare_input(v) for v in data)\n",
        "    elif isinstance(data, torch.Tensor):\n",
        "        kwargs = dict(device='cuda')\n",
        "        return data.to(**kwargs)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwdafJ59I7XS"
      },
      "source": [
        "### **[Req. 1-1]** 데이터 전처리 : ImageCaptionDataset 내 \\_\\_getitem\\_\\_() 메소드 구현\n",
        "본 실습은 이미지 경로를 텐서 형태의 이미지로, 캡션을 텐서 형태의 토큰으로 변환해주는 ImageCaptionDataset 클래스의 \\_\\_getitem\\_\\_() 메소드를 구현합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2PFzffHPkGQ"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_utils import no_init_weights\n",
        "\n",
        "with no_init_weights():\n",
        "    model = LimberGPTNeo.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
        "\n",
        "model.setup_multimodal(tokenizer, max_seq_len, encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, freeze_lm, freeze_img_encoder)\n",
        "\n",
        "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
        "transforms = model.transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdCug3_jhECV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# dataset\n",
        "CC3M = ImageCaptionDataset(\n",
        "        train_dataset_dir, tokenizer, transforms\n",
        "    )\n",
        "\n",
        "CC3M_dataloader = DataLoader(\n",
        "    CC3M,\n",
        "    batch_size=1,\n",
        "    collate_fn=ImageCaptionDataCollator(tokenizer)\n",
        ")\n",
        "test_inputs = next(iter(CC3M_dataloader))\n",
        "\n",
        "test_images = test_inputs[\"images\"]\n",
        "test_captions = test_inputs[\"captions\"]\n",
        "print(f\"test_images.shape : {test_images.shape}\")\n",
        "print(f\"test_captions.shape : {test_captions.shape}\")\n",
        "print(f\"test_captions : {test_captions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeHyiYGiNY05"
      },
      "source": [
        "### **[Req. 1-2]** 이미지를 입력 받아 이미지 프롬프트를 반환하는 ImagePrefix 클래스 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkkpqA90NYaa"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "test_images = test_images.to(device)\n",
        "test_captions = test_captions.to(device)\n",
        "\n",
        "with autocast(dtype=torch.float16):\n",
        "  test_image_representations=model.image_prefix.encoder(test_images)\n",
        "  test_image_prompts=model.image_prefix(test_images)\n",
        "\n",
        "print(f\"test_image_representations.shape : {test_image_representations.shape}\")\n",
        "print(f\"test_image_prompts.shape : {test_image_prompts.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLy4-gxiUhmg"
      },
      "source": [
        "\n",
        "\n",
        "### **[Req. 1-3]** LimberGPTJ 내 setmultimodal 함수 구현\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUaQtfI1V7op"
      },
      "source": [
        "### **[Req. 1-4]** LimberGPTJ 내 make_input_embeddings 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_VpMwznU-MM"
      },
      "outputs": [],
      "source": [
        "with autocast(dtype=torch.float16):\n",
        "  test_image_prompts, test_caption_embeddings, test_input_embeddings =model.make_input_embeddings(test_images, test_captions)\n",
        "\n",
        "print(f\"test_image_prompts.shape : {test_image_prompts.shape}\")\n",
        "print(f\"test_caption_embeddings.shape : {test_caption_embeddings.shape}\")\n",
        "print(f\"test_input_embeddings.shape : {test_input_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1ZwA2_EV_sn"
      },
      "source": [
        "### **[Req. 1-5]** LimberGPTJ 내 build_labels_for_training 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9zIbXohWSOB"
      },
      "outputs": [],
      "source": [
        "with autocast(dtype=torch.float16):\n",
        "  test_labels = model.build_labels_for_training(test_image_prompts,test_captions)\n",
        "\n",
        "print(f\"test_labels[0] : {test_labels[0]}\")\n",
        "print(f\"test_labels.shape : {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1R0hJIxPezc"
      },
      "source": [
        "## 3. LimberGPTJ 학습 스크립트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-gaMt0If16I"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from torch.cuda.amp import autocast\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "save_dir = os.path.join(save, exp_name)\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Dataset 정의\n",
        "train_dataset = ImageCaptionDataset(\n",
        "        train_dataset_dir, tokenizer, transforms\n",
        "    )\n",
        "\n",
        "# Dataloader 정의\n",
        "generator = torch.Generator()\n",
        "seed = 2024\n",
        "generator.manual_seed(seed)\n",
        "\n",
        "train_sampler = RandomSampler(train_dataset, generator=generator)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    sampler=train_sampler,\n",
        "    collate_fn=ImageCaptionDataCollator(tokenizer),\n",
        "    drop_last=True,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Optimizer 정의\n",
        "trainable_parameters = configure_param_groups(model)\n",
        "optimizer = AdamW(\n",
        "        trainable_parameters,\n",
        "        lr,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "scheduler = WarmupDecayLR(optimizer, lr_decay_iters)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "print(\"Start Training!\")\n",
        "model.train()\n",
        "model.gradient_checkpointing_enable()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "max_steps = train_steps\n",
        "total_train_batch_size = train_batch_size * gradient_accumulation_steps\n",
        "steps_in_epoch = len(train_dataloader)\n",
        "num_update_steps_per_epoch = steps_in_epoch // gradient_accumulation_steps\n",
        "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
        "num_train_epochs = max_steps // num_update_steps_per_epoch + int(\n",
        "    max_steps % num_update_steps_per_epoch > 0\n",
        ")\n",
        "num_train_samples = max_steps * total_train_batch_size\n",
        "\n",
        "print(\"***** Running training *****\")\n",
        "print(f\"  Num examples = {len(train_dataloader.dataset)}\")\n",
        "print(f\"  Num Epochs = {num_train_epochs}\")\n",
        "print(f\"  Total train batch size = {total_train_batch_size}\")\n",
        "print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
        "print(f\"  Total optimization steps = {max_steps}\")\n",
        "\n",
        "global_step=0\n",
        "tr_loss = torch.tensor(0.0).to(device)\n",
        "model.zero_grad()\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "  for step, inputs in enumerate(train_dataloader):\n",
        "    inputs = _prepare_input(inputs)\n",
        "    with autocast(dtype=torch.float16):\n",
        "        outputs = model(**inputs)\n",
        "        # Loss 정의\n",
        "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "    loss = loss / gradient_accumulation_steps\n",
        "    # Backpropagation\n",
        "    scaler.scale(loss).backward()\n",
        "    tr_loss_step = loss.detach()\n",
        "    tr_loss += tr_loss_step\n",
        "\n",
        "    if (step + 1) % gradient_accumulation_steps == 0:\n",
        "      scaler.unscale_(optimizer)\n",
        "      nn.utils.clip_grad_norm_(\n",
        "          model.parameters(),\n",
        "          1.0,\n",
        "      )\n",
        "      # Optimizer step\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      model.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "      # 현재 step number에 해당하는 가중치(weights)와 편향(bias)를 저장합니다.\n",
        "      if global_step % 10 == 0 :\n",
        "\n",
        "        # 모델의 state_dict을 가져옵니다.\n",
        "        state_dict = model.state_dict()\n",
        "        # 특정 layer(projection layer)의 가중치(weights)와 편향(bias)를 저장합니다.\n",
        "        selected_layer_params = {key: value for key, value in state_dict.items() if 'image_prefix.proj' in key}\n",
        "        save_path = f\"{save_dir}/step_{global_step}.ckpt\"\n",
        "        torch.save(selected_layer_params, save_path)\n",
        "        print(\"Model saved in \", save_path)\n",
        "\n",
        "      tr_loss_scaler = round(tr_loss.mean().item(), 4)\n",
        "      tr_loss -= tr_loss\n",
        "      print(f\"'loss': {tr_loss_scaler}, 'step': {global_step}\")\n",
        "\n",
        "print(f\"\\n\\nTraining completed in {time.time() - start_time:.5f} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpGtxThJTK1v"
      },
      "source": [
        "# 4. 시각-언어 멀티모달 LLM을 활용하여 이미지 캡션 생성\n",
        "\n",
        "LimberGPTJ 클래스의 함수들을 하나씩 구현하면서 Req. 1-6부터 Req. 1-8을 풀어봅니다. 구현에 필요한 부분은 다음과 같습니다.\n",
        "\n",
        "- LimberGPTJ 클래스  \n",
        "  - **Req. 1-6**: LimberGPTJ 내 preprocess_inputs 함수 구현\n",
        "  - **Req. 1-7**: LimberGPTJ 내 top_k_sampling 함수 구현\n",
        "  - **Req. 1-8**: LimberGPTJ 내 top_p_sampling 함수 구현  \n",
        "아래 셀부터 하나씩 따라가면서 Req. 1-6 와 Req. 1-8 에서 구현이 필요한 부분을 읽고, 다시 LimberGPTJ 클래스의 함수 파트로 돌아와서 해당 \"\"\"Write your code\"\"\" 부분을 구현해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6epeGG7oN5T0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torchtyping import TensorType\n",
        "from typing import Union, List\n",
        "\n",
        "############################################################################\n",
        "# Req 1-6: LimberGPTJ 내 preprocess_inputs 함수 구현                          #\n",
        "############################################################################\n",
        "def preprocess_inputs(model, img_path, text_prompt):\n",
        "    \"\"\"\n",
        "    :param img_path: 이미지 경로\n",
        "    :param text_prompt: 고정된 initial text prompt\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: 이미지와 고정된 initial text prompt를 언어 모델의 인풋으로 들어갈 임베딩으로 변환     #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) img_path를 텐서 형태의 이미지로, 고정된 initial text prompt를 텐서 형태의 토큰으로 변환\n",
        "    # 2) 이미지로부터 이미지 프롬프트를 얻고, 토큰으로부터 text prompt의 임베딩을 얻어 추론 시 언어 모델에\n",
        "    #    인풋으로 들어갈 임베딩 얻기\n",
        "\n",
        "    \"\"\"Write your code\"\"\"\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    return input_embedding\n",
        "\n",
        "############################################################################\n",
        "# Req 1-7: LimberGPTJ 내 top_k_sampling 함수 구현                             #\n",
        "############################################################################\n",
        "def top_k_sampling(model,logits, top_k, temperature):\n",
        "    \"\"\"\n",
        "    :param logits: 모델이 예측한 다음 토큰의 probability (vocabulary에 있는 50258 토큰의 logit 값)\n",
        "    :param top_k: 다음 토큰 선택시, 상위 k개의 토큰을 후보로 고려\n",
        "    :param temperature: temperature scaling 값 (예측한 다음 토큰들의 확률 분포를 변형시킴.\n",
        "                        0에 가까울 수록 확률 분포가 날카로워지며, 1에 가까울 수록 확률 분포가 평평해짐.)\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: 언어 모델이 예측한 다음 단어에 대한 logits(모든 토큰의 logit 값)로부터              #\n",
        "    # Top-K sampling 방식을 사용하여 다음 토큰을 샘플링                                    #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) torch.topk 함수를 사용하여 logits로부터 상위 k개 토큰의 logit과 index를 추출\n",
        "    # 2) torch.full_like 함수를 사용하여 -infinite 값으로 채워진 logits 크기의 텐서 생성\n",
        "    # 3) 1에서 구한 상위 k개 토큰의 logit을 2에서 생성한 텐서의 해당 index에 assign\n",
        "    # 4) torch.nn.functional의 softmax 함수를 사용하여 다음 토큰의 probability 계산\n",
        "    # 5) torch.multinomial 함수를 사용하여 probability로부터 한 개 토큰을 샘플링 실행 결과 예시\n",
        "\n",
        "    \"\"\"Write your code\"\"\"\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    return next_token\n",
        "\n",
        "############################################################################\n",
        "# Req 1-8: LimberGPTJ 내 top_p_sampling 함수 구현                             #\n",
        "############################################################################\n",
        "def top_p_sampling(model,logits, top_p, temperature):\n",
        "    \"\"\"\n",
        "    :param logits: 모델이 예측한 다음 토큰의 probability (vocabulary에 있는 50258 토큰의 logit 값)\n",
        "    :param top_p: 다음 토큰 선택시, 누적 확률이 p 이상이 되는 최소한의 토큰 집합을 후보로 고려\n",
        "    :param temperature: temperature scaling 값 (예측한 다음 토큰들의 확률 분포를 변형시킴.\n",
        "                        0에 가까울 수록 확률 분포가 날카로워지며, 1에 가까울 수록 확률 분포가 평평해짐.)\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: 언어 모델이 예측한 다음 단어에 대한 logits(모든 토큰의 logit 값)로부터              #\n",
        "    # Top-P sampling 방식을 사용하여 다음 토큰을 샘플링                                    #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) torch.sort 함수를 사용하여 logits를 내림차순으로 정렬\n",
        "    # 2) torch.cumsum 함수를 사용하여 logits의 누적 분포 계산\n",
        "    # 3) 누적 분포를 바탕으로 해당 인덱스의 토큰이 후보에서 제외하는지 여부를 담은 텐서 생성\n",
        "    #    i. 누적 확률이 p 이상이 되는 index에 True 값 할당\n",
        "    #    ii. 누적 확률이 p 이상이 되는 최소한의 토큰 집합이므로, 위에서 할당한 값들을 오른쪽으로 한 칸 씩 이동\n",
        "    #    iii. 확률이 가장 높은 0번째 index는 항상 False 값 할당\n",
        "    # 4) 후보에서 제외되는 토큰의 logit은 -infinite로 할당\n",
        "    # 5) torch.nn.functional의 softmax 함수를 사용하여 다음 토큰의 probability 계산\n",
        "    # 6) torch.multinomial 함수를 사용하여 probability로부터 한 개 토큰을 샘플링\n",
        "\n",
        "    \"\"\"Write your code\"\"\"\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    return next_token\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model: \"GPTNeo\",\n",
        "    embeddings: TensorType[\"b\", \"s\", \"d\"],\n",
        "    max_steps: int = 100,\n",
        "    top_k: int = 0,\n",
        "    top_p: float = 0.9,\n",
        "    decode: bool = True,\n",
        "    temperature: float = 0.7,\n",
        "\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    embedding에 대한 캡션을 생성합니다.\n",
        "\n",
        "    :param model: 캡션 생성에 쓰일 모델\n",
        "    :param embeddings: 캡션을 생성할 임베딩\n",
        "    :param max_steps: 캡션을 생성할 때 최대 step 수\n",
        "    :param top_k: Top k 샘플링에 사용할 값, 0이면 샘플링이 사용되지 않음\n",
        "    :param top_p: Top p 샘플링에 사용할 값, 0이면 샘플링이 사용되지 않음\n",
        "    :param decode: 토큰을 decode하여 텍스트 형태로 반환할지 또는 토큰을 반환할지 여부\n",
        "    :param temperature: temperature scaling 값 (예측한 다음 토큰들의 확률 분포를 변형시킴.\n",
        "                        0에 가까울 수록 확률 분포가 날카로워지며, 1에 가까울 수록 확률 분포가 평평해짐.)\n",
        "    \"\"\"\n",
        "\n",
        "    eos_token = model.eos_token\n",
        "    b, s, _ = embeddings.shape\n",
        "    past_key_values = None\n",
        "\n",
        "    # 이미지 토큰으로 output을 초기화\n",
        "    out = torch.zeros((b, s), dtype=torch.long).to(model.device) + model.image_token\n",
        "\n",
        "    # sampling\n",
        "    for i in range(max_steps):\n",
        "        if i == 0:\n",
        "            # 초기 input\n",
        "            outputs = super(type(model),model).forward(\n",
        "                inputs_embeds=embeddings,\n",
        "                use_cache=True,\n",
        "                past_key_values=past_key_values,\n",
        "            )\n",
        "        else:\n",
        "            # 과거 key와 value를 캐싱하여 마지막 토큰만 사용\n",
        "            outputs = super(type(model),model).forward(\n",
        "                input_ids=out[:, -1:], use_cache=True, past_key_values=past_key_values\n",
        "            )\n",
        "\n",
        "        logits = outputs.logits[:, -1, :].float()\n",
        "        past_key_values = outputs.past_key_values\n",
        "\n",
        "        if top_k > 0:\n",
        "          # Top K sampling\n",
        "          next_token = model.top_k_sampling(logits,top_k,temperature)\n",
        "        if top_p > 0:\n",
        "          # Nucleus sampling\n",
        "          next_token = model.top_p_sampling(logits,top_p,temperature)\n",
        "\n",
        "        out = torch.cat((out, next_token), dim=-1)\n",
        "\n",
        "        if eos_token is not None and (next_token == eos_token).all():\n",
        "            break\n",
        "\n",
        "    if decode:\n",
        "        captions = []\n",
        "        for b in out:\n",
        "          eos_index = (b == eos_token).nonzero()\n",
        "          if eos_index.any():\n",
        "            b[eos_index[0] :] = eos_token\n",
        "          b = b.tolist()\n",
        "          b=[i for i in b if (not i == model.image_token) and (not i == eos_token)]\n",
        "          # 토큰을 decode하여 텍스트 형태로 변환\n",
        "          caption = model.tokenizer.decode(b)\n",
        "          captions.append(caption)\n",
        "        out = captions\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76j0B31vt6nT"
      },
      "source": [
        "**Test Image:**  \n",
        "![test_image.png](https://ak2.picdn.net/shutterstock/videos/18488392/thumb/9.jpg)  \n",
        "(그림 출처: https://ak2.picdn.net/shutterstock/videos/18488392/thumb/9.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsVBixc6lzpm"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# 제공된 미리 학습된 모델 가중치를 로드합니다.\n",
        "limber_proj_path = \"./limber/checkpoints/gpt_neo_pretrained_weights.ckpt\"\n",
        "proj_ckpt = torch.load(limber_proj_path)\n",
        "proj_ckpt = {key.split(\"image_prefix.proj.\")[1]: value for key, value in proj_ckpt.items()}\n",
        "model.image_prefix.proj.load_state_dict(proj_ckpt)\n",
        "model = model.cuda().half()\n",
        "\n",
        "start_time = time.time()\n",
        "img_path = \"./limber/cc3m/image_captioning.jpg\"\n",
        "model.eval()\n",
        "print(\"Start inference...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FzYpYoA7-MS"
      },
      "source": [
        "### **[Req. 1-6]** LimberGPTJ 내 preprocess_inputs 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lZiXfza8Km3"
      },
      "outputs": [],
      "source": [
        "embeddings = model.preprocess_inputs(img_path,'A picture of')\n",
        "print(f\"embeddings.shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38kTGGwc8whp"
      },
      "source": [
        "### **[Req. 1-7]** LimberGPTJ 내 top_k_sampling 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1i_6QKu8N50"
      },
      "outputs": [],
      "source": [
        "caption = model.generate(embeddings=embeddings,top_k=3,temperature=0.7)\n",
        "print(\"\\nOutput: \")\n",
        "print(caption[0].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrIcyObX8zti"
      },
      "source": [
        "### **[Req. 1-8]** LimberGPTJ 내 top_p_sampling 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS2c5KuI8sFy"
      },
      "outputs": [],
      "source": [
        "caption = model.generate(embeddings=embeddings,top_p=0.9,temperature=0.7)\n",
        "print(\"\\nOutput: \")\n",
        "print(caption[0].strip())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
