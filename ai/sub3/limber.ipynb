{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQjM0bpnL4oa"
      },
      "source": [
        "## ê³¼ì œ ëª©í‘œ\n",
        "1. LiMBeR ëª¨ë¸ Architectureì˜ ì •ì„±ì  ì´í•´\n",
        "2. í•™ìŠµëœ CLIPê³¼ LLMì„ í™œìš©í•˜ì—¬ ëŒ€ê·œëª¨ ì‹œê°-ì–¸ì–´ ë©€í‹°ëª¨ë‹¬ LLMì„ êµ¬ì¶•\n",
        "3. ì‹œê°-ì–¸ì–´ ë©€í‹°ëª¨ë‹¬ LLM í•™ìŠµ\n",
        "3. í•™ìŠµëœ ì‹œê°-ì–¸ì–´ ë©€í‹°ëª¨ë‹¬ LLMì„ í™œìš©í•œ Image Captioning Task\n",
        "\n",
        "## ì°¸ê³  ìë£Œ\n",
        " - Paper : [LiMBeR](https://arxiv.org/abs/2209.15162)\n",
        " - Code : [LiMBeR github](https://github.com/jmerullo/limber), [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\n",
        " - Dataset : [CC3M](https://github.com/google-research-datasets/conceptual-captions)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgsWhnkiSlq7"
      },
      "source": [
        "## 0. í™˜ê²½ ì„¸íŒ…\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "í•¨ê»˜ í¬í•¨ëœ limber.zip íŒŒì¼ì„ ì••ì¶• í•´ì œí•˜ì—¬ í•´ë‹¹ limber í´ë”ë¥¼ sub3 í´ë” ì•ˆì— ë„£ì–´ì¤ë‹ˆë‹¤.  \n",
        "\n",
        "ë””ë ‰í† ë¦¬ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- ë””ë ‰í† ë¦¬ êµ¬ì¡°  \n",
        "      |-- project/  \n",
        "      |   |-- sub1/  \n",
        "      |   |-- sub2/   \n",
        "      |   |-- sub3/  \n",
        "      |   |   |-- limber/  \n",
        "      |   |   |-- limber.ipynb   \n",
        "      |   |   |-- vqa.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDmsHw7WIx0Q"
      },
      "source": [
        "Colabì—ì„œ ì§„í–‰í•˜ëŠ” ê²½ìš°, ì•„ë˜ ì½”ë“œ ì£¼ì„ì„ í•´ì œí•˜ê³  ì…€ì„ ì‹¤í–‰ì‹œì¼œ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ë¥¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGCTryKHTFul"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEi6N2rbIx0R"
      },
      "source": [
        "Colabì—ì„œ ì§„í–‰í•˜ëŠ” ê²½ìš°, êµ¬ê¸€ ë“œë¼ì´ë¸Œì— project ë””ë ‰í† ë¦¬ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.   \n",
        "ì´í›„, ì•„ë˜ ì½”ë“œ ì£¼ì„ì„ í•´ì œí•˜ê³  ì…€ì„ ì‹¤í–‰ì‹œì¼œ í˜„ì¬ ì‘ì—…ì¤‘ì¸ ë””ë ‰í† ë¦¬ë¥¼ ì´ë™í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEnOuTwfTGeq"
      },
      "outputs": [],
      "source": [
        "# cd \"/content/drive/MyDrive/project/sub3/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpK-2bX7T2hw"
      },
      "source": [
        "í•„ìš”í•œ pakageë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkqEWt_z0OHS"
      },
      "source": [
        "      pip install transformers\n",
        "      pip install https://github.com/openai/CLIP/archive/master.zip\n",
        "      pip install torchtyping\n",
        "      pip install einops\n",
        "      pip install pyhelpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuoE8wOW1rh9"
      },
      "source": [
        "Colabì—ì„œ ì§„í–‰í•˜ëŠ” ê²½ìš°, ì•„ë˜ ì½”ë“œ ì£¼ì„ì„ í•´ì œí•˜ê³  ì…€ì„ ì‹¤í–‰ì‹œì¼œ í•„ìš”í•œ pakageë“¤ì„ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgr9rxvS-UtQ"
      },
      "outputs": [],
      "source": [
        "# !pip install -qq transformers\n",
        "# !pip install -qq https://github.com/openai/CLIP/archive/master.zip\n",
        "# !pip install -qq torchtyping\n",
        "# !pip install -qq einops\n",
        "# !pip install -qq pyhelpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh0KAG8h-UtQ"
      },
      "source": [
        "seedë¥¼ ê³ ì •í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPTyd92L-UtR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random_seed = 1111\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG5LdNxdCVoG"
      },
      "source": [
        "## 1. Image Captioning Data ì „ì²˜ë¦¬  \n",
        "\n",
        "**1-1. CC3M(Conceptual Captions) Dataset ì„¤ëª…**    \n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” [CC3M](https://github.com/google-research-datasets/conceptual-captions) ë°ì´í„°ì…‹ì„ í›ˆë ¨ ë° ì¶”ë¡ ì— ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. CC3Mì€ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹œìŠ¤í…œì˜ í›ˆë ¨ ë° í‰ê°€ë¥¼ ìœ„í•´ ì„¤ê³„ëœ 300ë§Œ ê°œ ì´ìƒì˜ (ì´ë¯¸ì§€ URL, ìº¡ì…˜) ìŒì„ í¬í•¨í•˜ëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
        "\n",
        "ë‹¤ìŒì€ CC3M ë°ì´í„°ì…‹ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ë¯¸ì§€:  \n",
        "\n",
        "![image.png](https://thumb7.shutterstock.com/display_pic_with_logo/1350382/147082805/stock-vector-white-swan-on-the-blue-background-147082805.jpg)  \n",
        "(ê·¸ë¦¼ ì¶œì²˜: https://thumb7.shutterstock.com/display_pic_with_logo/1350382/147082805/stock-vector-white-swan-on-the-blue-background-147082805.jpg)\n",
        "\n",
        "ìº¡ì…˜:\n",
        "white swan on the blue background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwCX5ixLIVmr"
      },
      "source": [
        "ì´ë²ˆ ì‹¤ìŠµì€ ì €í¬ê°€ ì œê³µí•˜ëŠ” ì†ŒëŸ‰ì˜ CC3M ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ë¡œ ì‹¤ìŠµì„ ì§„í–‰í•˜ê³ ì í•˜ëŠ” ë¶„ë“¤ì€ ì•„ë˜ 1-2 ì§€ì¹¨ì„ ë”°ë¼ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ë§Œë“  í›„ ì§„í–‰í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mOWOzNDLSnk"
      },
      "source": [
        "**1-2. CC3M ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiMOl5pTt8-q"
      },
      "source": [
        "- ë‹¤ìš´ë¡œë“œ ë§í¬ : https://huggingface.co/datasets/zatepyakin/cc3m_min256_max512/tree/main\n",
        "\n",
        "- ë‹¤ìš´ë¡œë“œ ì˜ˆì‹œ\n",
        "\n",
        "      mkdir cc3m && cd cc3m\n",
        "      mkdir training && cd training\n",
        "\n",
        "      wget https://huggingface.co/datasets/zatepyakin/cc3m_min256_max512/resolve/main/00000.tar\n",
        "      tar -xvf \"00000.tar\"\n",
        "\n",
        "      mkdir images\n",
        "      mv *.jpg images\n",
        "\n",
        "      mkdir image_data\n",
        "      mv *.json image_data\n",
        "\n",
        "      rm *.txt\n",
        "\n",
        "- ë””ë ‰í† ë¦¬ êµ¬ì¡°  \n",
        "      |-- cc3m  \n",
        "      |   |-- train  \n",
        "      |   |   |-- image_data  \n",
        "      |   |   |   |-- 000000000.json    \n",
        "      |   |   |   |-- 000000001.json   \n",
        "      |   |   |   |-- ...  \n",
        "      |   |   |-- images  \n",
        "      |   |   |   |-- 000000000.jpg    \n",
        "      |   |   |   |-- 000000001.jpg   \n",
        "      |   |   |   |-- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEWJvjK0FHbg"
      },
      "source": [
        "**1-3. Tokenizer ì •ì˜**  \n",
        "í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ Language Modelì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” í† í° í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì´ë¥¼ ìœ„í•œ ì¼ë ¨ì˜ ê·œì¹™ì— ë”°ë¼ í…ìŠ¤íŠ¸ë¥¼ í† í° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì œê³µí•©ë‹ˆë‹¤.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxKZYqnG1hV"
      },
      "source": [
        "Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ tokenizerë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUVIqZKnGSo6"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” pretrained tokenizerë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "# pad token idë¥¼ eos token idë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# ëª¨ë¸ì´ paddingì„ ì˜¤ë¥¸ìª½ì— ì ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
        "tokenizer.padding_side = \"right\"\n",
        "# ëª¨ë¸ì˜ ì…ë ¥ì— ëŒ€í•œ ìµœëŒ€ ê¸¸ì´(í† í° ìˆ˜ë¡œ ì¸¡ì •)ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "tokenizer.model_max_length = 2048\n",
        "# '<|image|>' ë¬¸ìì—´ì„ í´ë˜ìŠ¤ í† í°ìœ¼ë¡œ ì„¤ì •í•˜ê³  tokenizerì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "tokenizer.add_special_tokens(\n",
        "            {'cls_token': '<|image|>'}\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEjooBbVGSpG"
      },
      "source": [
        "ì •ì˜í•œ tokenizerì˜ encode í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì €ì— ì „ë‹¬í•´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê²°ê³¼ë¥¼ ë³´ë©´, ì¼ë ¨ì˜ ê·œì¹™ì— ë”°ë¼ í…ìŠ¤íŠ¸ê°€ í† í° ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcvgmMviGSpG"
      },
      "outputs": [],
      "source": [
        "encoded_input = tokenizer.encode(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfGh6HpTGSpG"
      },
      "source": [
        "í† í° ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ëŠ” tokenizerì˜ decode í•¨ìˆ˜ë¥¼ í†µí•´ ì›ë˜ì˜ textë¡œ ë°˜í™˜ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s6RQoANGSpG"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(encoded_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cPXuwE4JV-n"
      },
      "source": [
        "ì›ë˜ì˜ textê°€ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
        "ì´ì œ tokenizerì— ì €ì¥ë˜ì–´ìˆëŠ” vocabularyë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì¶œë ¥í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  \n",
        "í‘œí˜„ ê°€ëŠ¥í•œ vocabularyëŠ” ìš°ë¦¬ê°€ ìœ„ì—ì„œ tokenizerì— ì¶”ê°€í•œ '<|image|>' í† í°ì„ í¬í•¨í•˜ì—¬ ì´ 50258ê°œ ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHPvil8oN9_R"
      },
      "outputs": [],
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "\n",
        "print(list(vocab.items())[:10])\n",
        "print(f\"len(vocab) : {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BVa_pTxQcIg"
      },
      "source": [
        "Vocabì˜ ê°€ì¥ ë§ˆì§€ë§‰ì— ìˆëŠ” í† í°ì´ ìš°ë¦¬ê°€ ì¶”ê°€í•œ '<|image|>' í† í°ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ_2Pci_P23X"
      },
      "outputs": [],
      "source": [
        "print(vocab['<|image|>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxjhVliAAciS"
      },
      "source": [
        "**1-4. ImageCaptionDataset class ì •ì˜**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ddhehEIhGx"
      },
      "source": [
        "CC3M ë°ì´í„°ì™€ tokenizerê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ, ì´ì œ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì˜ˆìƒí•˜ëŠ” ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬í•´ì£¼ëŠ” custom dataset classë¥¼ ì •ì˜í•©ë‹ˆë‹¤.  \n",
        "\n",
        "ImageCaptionDataset í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ë©´ì„œ Req. 1-1ì„ í’€ì–´ë´…ë‹ˆë‹¤. êµ¬í˜„ì— í•„ìš”í•œ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. í•´ë‹¹ \"\"\"Write your code\"\"\" ë¶€ë¶„ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ImageCaptionDataset í´ë˜ìŠ¤\n",
        "  - **Req. 1-1**: ë°ì´í„° ì „ì²˜ë¦¬ : ImageCaptionDataset ë‚´ \\_\\_getitem\\_\\_() ë©”ì†Œë“œ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09UQpKL8UCBK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torchtyping import TensorType\n",
        "from typing import Tuple\n",
        "import os\n",
        "from PIL import Image\n",
        "from pyhelpers.store import load_json\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, data_dir, tokenizer=None, transforms=None, seq_len=2048\n",
        "    ):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        paths = []\n",
        "        img_data_dir = self.data_dir / \"image_data\"\n",
        "        for p in tqdm(Path(img_data_dir).glob(f\"*.json\"), desc=f\"loading dataset paths from {str(img_data_dir)}\"):\n",
        "          paths.append(p)\n",
        "        self.paths = sorted(paths)\n",
        "        print(\"LENGTH OF DATA\", len(self.paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-1: ë°ì´í„° ì „ì²˜ë¦¬ : ImageCaptionDataset ë‚´ __getitem__() ë©”ì†Œë“œ êµ¬í˜„       #\n",
        "    ############################################################################\n",
        "    def __getitem__(\n",
        "        self, idx\n",
        "    ) -> Tuple[TensorType[\"b\", \"c\", \"h\", \"w\"], TensorType[\"b\", \"s\"]]:\n",
        "\n",
        "        img_data = load_json(self.paths[idx])\n",
        "        img_path = os.path.join(self.data_dir, \"images\", img_data['key']+\".jpg\")\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: img_pathë¥¼ í…ì„œ í˜•íƒœì˜ ì´ë¯¸ì§€ë¡œ, captionì„ transformers libraryì˜ tokenizerë¥¼ #\n",
        "        # í†µí•´ í…ì„œ í˜•íƒœì˜ í† í°ìœ¼ë¡œ ë³€ê²½                                                      #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) PIL libraryì˜ Image.open() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ img_pathë¡œë¶€í„° RGB í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ\n",
        "        #    PIL.Image ì´ë¯¸ì§€ ê°ì²´ ì–»ê¸°\n",
        "        # 2) PIL.Image ì´ë¯¸ì§€ ê°ì²´ë¥¼ ì‚¬ìš©ì ì •ì˜ëœ self.transformsë¥¼ ì´ìš©í•˜ì—¬ ì „ì²˜ë¦¬í•˜ì—¬\n",
        "        #    [channel, height, width] í˜•íƒœì˜ í…ì„œë¡œ ë³€í™˜\n",
        "        # 3) Transformers libraryì˜ tokenizerì—ì„œ ì •ì˜ëœ encode() í•¨ìˆ˜ë¡œ captionì„\n",
        "        #    í…ì„œ í˜•íƒœì˜ tokenìœ¼ë¡œ ë³€í™˜\n",
        "\n",
        "        img = \"\"\"Write your code\"\"\"\n",
        "        img_tensor = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        caption = img_data[\"caption\"]\n",
        "        token_tensor = self.tokenizer.encode(\n",
        "            \"\"\"Write your code\"\"\",\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.seq_len,\n",
        "            padding=\"longest\",\n",
        "            truncation=True,\n",
        "        ).squeeze()\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return {'images':img_tensor, 'captions':token_tensor}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPQo_vU3PQ6I"
      },
      "source": [
        "# 2. LiMBeR êµ¬í˜„"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqSEW96UKyq8"
      },
      "source": [
        "LiMBeR(Linearly Mapping Between Representation spaces) ëª¨ë¸ì˜ ê¸°ë³¸ ì ‘ê·¼ ë°©ì‹ì€ ì‚¬ì „ í›ˆë ¨ëœ ì´ë¯¸ì§€ ì¸ì½”ë”ì˜ hidden size $h_I$ë¥¼ ì–¸ì–´ ëª¨ë¸ì˜ input spaceë¡œ íˆ¬ì˜í•˜ê¸° ìœ„í•´ linear layer Pë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jchK4RN8QrAY"
      },
      "source": [
        "**2-1. Image Encoder**  \n",
        "ì´ë¯¸ì§€ ì¸ì½”ë”ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” $h_I$ ì°¨ì›ì˜ ì´ë¯¸ì§€ í‘œí˜„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "**2-2. Projection Layer**  \n",
        "Projection $P$ëŠ” ì´ë¯¸ì§€ í‘œí˜„ì„ $e_L$ * $k$ ì‹œí€€ìŠ¤ì˜ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ íˆ¬ì˜í•©ë‹ˆë‹¤. ì´ë¥¼ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¼ê³  ì§€ì¹­í•©ë‹ˆë‹¤.\n",
        "\n",
        "**2-3. Text Decoder (Large Language Model)**  \n",
        "ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì„ promptingí•¨ìœ¼ë¡œì¨, ì–¸ì–´ ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ìº¡ì…˜ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.    \n",
        "\n",
        "Projection Layerì˜ ì–‘ìª½ì— ìˆëŠ” ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ ì–¸ì–´ ëª¨ë¸ì„ ë™ê²°í•˜ì—¬ í•™ìŠµëœ íŒŒë¼ë¯¸í„°ê°€ í”„ë¡œì ì…˜ ë ˆì´ì–´ì—ë§Œ ì¡´ì¬í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ë³¸ ì‹¤ìŠµì—ì„œëŠ” ì´ë¯¸ì§€ ì¸ì½”ë”ë¡œ **CLIP RN50x16**ë¥¼ ì‚¬ìš©í•˜ë©° **$k$=144, $h_I$=3072, $e_L$=2048** ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.  \n",
        "ë˜í•œ ë…¼ë¬¸ì—ì„œëŠ” ì–¸ì–´ ëª¨ë¸ë¡œ 60ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ decoder-only GPT-J ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë‚˜, Colabì˜ ìì› ì œí•œìœ¼ë¡œ ì¸í•´ ë³¸ ì‹¤ìŠµì—ì„œëŠ” 13ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ **decoder-only GPT-Neo** ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY-vWh_kLcZj"
      },
      "source": [
        "ImagePrefix í´ë˜ìŠ¤ì™€ LimberGPTJ í´ë˜ìŠ¤ì˜ í•¨ìˆ˜ë“¤ì„ í•˜ë‚˜ì”© êµ¬í˜„í•˜ë©´ì„œ Req. 1-2 ë¶€í„° Req. 1-5ë¥¼ í’€ì–´ë´…ë‹ˆë‹¤. êµ¬í˜„ì— í•„ìš”í•œ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- ImagePrefix í´ë˜ìŠ¤  \n",
        "  - **Req. 1-2**: ì´ë¯¸ì§€ë¥¼ ì…ë ¥ ë°›ì•„ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ImagePrefix í´ë˜ìŠ¤ êµ¬í˜„\n",
        "- LimberGPTJ í´ë˜ìŠ¤  \n",
        "  - **Req. 1-3**: LimberGPTJ ë‚´ setmultimodal í•¨ìˆ˜ êµ¬í˜„\n",
        "  - **Req. 1-4**: LimberGPTJ ë‚´ make_input_embeddings í•¨ìˆ˜ êµ¬í˜„\n",
        "  - **Req. 1-5**: LimberGPTJ ë‚´ build_labels_for_training í•¨ìˆ˜ êµ¬í˜„\n",
        "\n",
        "ì•„ë˜ ì…€ë¶€í„° í•˜ë‚˜ì”© ë”°ë¼ê°€ë©´ì„œ Req. 1-2 ë¶€í„° Req. 1-5 ì—ì„œ êµ¬í˜„ì´ í•„ìš”í•œ ë¶€ë¶„ì„ ì½ê³ , ë‹¤ì‹œ ImagePrefix í´ë˜ìŠ¤ ë˜ëŠ” LimberGPTJ í´ë˜ìŠ¤ë¡œ ëŒì•„ì™€ì„œ í•´ë‹¹ \"\"\"Write your code\"\"\" ë¶€ë¶„ì„ êµ¬í˜„í•´ë´…ë‹ˆë‹¤.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6lHXEqHHIWM"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Union\n",
        "import torch.nn as nn\n",
        "import clip\n",
        "from einops import rearrange\n",
        "\n",
        "############################################################################\n",
        "# Req 1-2: ì´ë¯¸ì§€ë¥¼ ì…ë ¥ ë°›ì•„ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ImagePrefix í´ë˜ìŠ¤ êµ¬í˜„         #\n",
        "############################################################################\n",
        "class ImagePrefix(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì–¸ì–´ ëª¨ë¸ì˜ word embeddingê³¼ ë™ì¼í•œ h_I ì°¨ì›ì˜ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "    :param encoder_out_dim: ì´ë¯¸ì§€ ì¸ì½”ë”ì˜ ì¶œë ¥ ì°¨ì› (=h_I)\n",
        "    :param lm_out_dim: ì–¸ì–´ ëª¨ë¸ì˜ ì…ë ¥ ë° ì¶œë ¥ ì°¨ì› (=e_L)\n",
        "    :param out_seq_len: ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ (=k)\n",
        "    :param image_embed_dropout_prob: ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì— ì ìš©í•  dropout probability\n",
        "    :param device: ëª¨ë¸ì„ ì‹¤í–‰ í•  ì¥ì¹˜\n",
        "    \"\"\"\n",
        "\n",
        "    ################################################################################\n",
        "    # TODO: í…ì„œ í˜•íƒœ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ CLIPì˜ image encoderë¥¼ í†µí•´ ì´ë¯¸ì§€ í‘œí˜„ì„ ì–»ê³           #\n",
        "    # projection layerë¥¼ í†µí•´ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ íˆ¬ì˜í•˜ëŠ” ImagePrefix í´ë˜ìŠ¤ êµ¬í˜„              #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) __init__() í•¨ìˆ˜ì—ì„œ pretrained CLIP image encoder ê°€ì ¸ì˜¤ê¸°\n",
        "    # 2) __init__() í•¨ìˆ˜ì—ì„œ projection layer ì •ì˜\n",
        "    # 3) __forward__() í•¨ìˆ˜ì—ì„œ pretrained CLIP image encoderë¥¼ ì‚¬ìš©í•˜ì—¬ imageë¡œë¶€í„° ì´ë¯¸ì§€ í‘œí˜„ ì–»ê¸°\n",
        "    # 4) __forward__() í•¨ìˆ˜ì—ì„œ projection layerë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í‘œí˜„ì„ soft promptsë¡œ íˆ¬ì˜\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, device=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # pretrained CLIP image encoder ê°€ì ¸ì˜¤ê¸°\n",
        "        self.encoder = \"\"\"Write your code\"\"\"\n",
        "        self.encoder.attnpool = Lambda(\n",
        "            partial(rearrange, pattern=\"b d h w -> b (h w) d\")\n",
        "        )\n",
        "        # ì´ë¯¸ì§€ ì¸ì½”ë”ì˜ ì¶œë ¥ ì°¨ì› (=h_I)\n",
        "        self.encoder_out_dim = encoder_out_dim\n",
        "\n",
        "        # ì–¸ì–´ ëª¨ë¸ì˜ ì…ë ¥ ë° ì¶œë ¥ ì°¨ì› (=e_L)\n",
        "        self.lm_out_dim = lm_out_dim\n",
        "\n",
        "        # ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ (=k)\n",
        "        self.out_seq_len = out_seq_len\n",
        "\n",
        "        # ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ ì°¨ì›\n",
        "        proj_out_dim = self.lm_out_dim\n",
        "\n",
        "        # Projection layer ì •ì˜\n",
        "        self.proj = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        self.dropout = nn.Dropout(image_embed_dropout_prob)\n",
        "\n",
        "    def forward(\n",
        "        self, x: TensorType[\"b\", \"c\", \"h\", \"w\"]\n",
        "    ) -> TensorType[\"b\", \"seq\", \"out_dim\"]:\n",
        "\n",
        "        image_feats = \"\"\"Write your code\"\"\"\n",
        "        soft_prompts = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì— dropout ì ìš©\n",
        "        soft_prompts = self.dropout(soft_prompts)\n",
        "\n",
        "        return soft_prompts\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sahzR_qvP6lt"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM\n",
        "from typing import List, Optional\n",
        "from torchtyping import TensorType\n",
        "from transformers.file_utils import ModelOutput\n",
        "from torchvision import transforms as T\n",
        "\n",
        "class LimberGPTNeo(GPTNeoForCausalLM):\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-3: LimberGPTJ ë‚´ setmultimodal í•¨ìˆ˜ êµ¬í˜„                              #\n",
        "    ############################################################################\n",
        "    def setup_multimodal(self, tokenizer, max_seq_len, encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, freeze_lm, freeze_img_encoder):\n",
        "\n",
        "        \"\"\"\n",
        "        :param tokenizer: tokenizer\n",
        "        :param max_seq_len: ì–¸ì–´ ëª¨ë¸ì˜ ì…ë ¥ì— ëŒ€í•œ ìµœëŒ€ ê¸¸ì´(í† í° ìˆ˜ë¡œ ì¸¡ì •)\n",
        "        :param encoder_out_dim: ì´ë¯¸ì§€ ì¸ì½”ë”ì˜ ì¶œë ¥ ì°¨ì› (=h_I)\n",
        "        :param lm_out_dim: ì–¸ì–´ ëª¨ë¸ì˜ ì…ë ¥ ë° ì¶œë ¥ ì°¨ì› (=e_L)\n",
        "        :param out_seq_len: ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ (=k)\n",
        "        :param image_embed_dropout_prob: ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì— ì ìš©í•  dropout probability\n",
        "        :param freeze_lm: ì–¸ì–´ ëª¨ë¸ì„ freezeí• ì§€ ì—¬ë¶€\n",
        "        :param freeze_img_encoder: ì´ë¯¸ì§€ ì¸ì½”ë”ë¥¼ freezeí• ì§€ ì—¬ë¶€\n",
        "        \"\"\"\n",
        "        ################################################################################\n",
        "        # TODO: Transformersì˜ GPTNeoForCausalLM í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ëŠ” LimberGPTJ í´ë˜ìŠ¤ì—ì„œ       #\n",
        "        # ImagePrefix í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•œë‹¤. Image encoderì™€ Language modelì€ freezeì‹œì¼œ  #\n",
        "        # Image Encoder(freeze) â€“ Projection Layer â€“ Language Model(freeze) êµ¬ì¡°ë¥¼ ë§Œë“ ë‹¤.#\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) ì–¸ì–´ ëª¨ë¸ freeze\n",
        "        # 2) ImagePrefix í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "        # 3) ì´ë¯¸ì§€ ì¸ì½”ë” freeze\n",
        "\n",
        "        self.seq_len = max_seq_len\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.image_token = self.tokenizer.cls_token_id\n",
        "        self.eos_token = self.tokenizer.eos_token_id\n",
        "        self.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        self.word_embedding = self.transformer.wte\n",
        "\n",
        "        # ì–¸ì–´ ëª¨ë¸ freeze\n",
        "        if freeze_lm:\n",
        "            \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # ImagePrefix í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "        self.image_prefix = \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ ì§€ì •\n",
        "        self.image_prompt_seq_len = self.image_prefix.out_seq_len\n",
        "        inp_rez = self.image_prefix.encoder.input_resolution\n",
        "\n",
        "        self.transforms = T.Compose(\n",
        "        [\n",
        "            T.Resize(inp_rez, interpolation=T.InterpolationMode.BICUBIC),\n",
        "            T.CenterCrop(inp_rez),\n",
        "            lambda image: image.convert(\"RGB\"),\n",
        "            T.ToTensor(),\n",
        "            add_batch_dim,\n",
        "            T.Normalize(\n",
        "                (0.48145466, 0.4578275, 0.40821073),\n",
        "                (0.26862954, 0.26130258, 0.27577711),\n",
        "            ),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "        # ì´ë¯¸ì§€ ì¸ì½”ë” freeze\n",
        "        if freeze_img_encoder:\n",
        "            \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-4: LimberGPTJ ë‚´ make_input_embeddings í•¨ìˆ˜ êµ¬í˜„                     #\n",
        "    ############################################################################\n",
        "    def make_input_embeddings(self,\n",
        "        images: TensorType[\"b\", \"c\", \"h\", \"w\"] = None,\n",
        "        captions: Optional[TensorType[\"b\", \"seq\"]] = None,\n",
        "        image_embeddings: TensorType[\"b\", \"s\", \"d\"] = None,):\n",
        "\n",
        "        \"\"\"\n",
        "        :param images: ì´ë¯¸ì§€\n",
        "        :param captions: ìº¡ì…˜\n",
        "        :param image_embeddings: ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ (inference ì‹œ ì‚¬ìš©)\n",
        "        \"\"\"\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: ImagePrefix í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ì…ë ¥ ì´ë¯¸ì§€ë¡œë¶€í„° ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ì–»ê³ ,      #\n",
        "        # ìº¡ì…˜ìœ¼ë¡œë¶€í„° ìº¡ì…˜ì˜ ì„ë² ë”©ì„ ì–»ì–´ í›ˆë ¨ ì‹œ LLMì— ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°ˆ ì„ë² ë”© ì–»ê¸°                   #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ\n",
        "        # 2) ìº¡ì…˜ ì„ë² ë”© ì¶”ì¶œ\n",
        "        # 3) í›ˆë ¨ ì‹œ LLMì— ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°ˆ ì„ë² ë”© ë§Œë“¤ê¸°\n",
        "\n",
        "        if image_embeddings is None:\n",
        "\n",
        "        \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return image_embeddings, caption_embeddings, input_embeddings\n",
        "\n",
        "    ############################################################################\n",
        "    # Req 1-5: LimberGPTJ ë‚´ build_labels_for_training í•¨ìˆ˜ êµ¬í˜„                 #\n",
        "    ############################################################################\n",
        "    def build_labels_for_training(self,\n",
        "        image_embeddings: TensorType[\"b\", \"s\", \"d\"] = None,\n",
        "        captions: Optional[TensorType[\"b\", \"seq\"]] = None,):\n",
        "\n",
        "        \"\"\"\n",
        "        :param image_embeddings: ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸\n",
        "        :param captions: ìº¡ì…˜\n",
        "        \"\"\"\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: ì´ë¯¸ì§€ì™€ ìº¡ì…˜ìœ¼ë¡œë¶€í„° ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ ë¼ë²¨ì„ ë§Œë“ ë‹¤.                        #\n",
        "        # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ì™€, textì—ì„œ ì²« eos í† í° ì´í›„ ë°˜ë³µë˜ëŠ” eos í† í°ì— ëŒ€í•´ì„œëŠ” lossê°€            #\n",
        "        # ê³„ì‚°ë˜ì§€ ì•Šì•„ì•¼ í•˜ê¸° ë•Œë¬¸ì— í•´ë‹¹í•˜ëŠ” labelì„ -100ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì–´ masking í•´ì¤€ë‹¤.            #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ì˜ length ë§Œí¼ì˜ labelì„ -100ìœ¼ë¡œ ì„¤ì •í•œë‹¤.\n",
        "        # 2) ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ì˜ labelê³¼ captionì„ concatí•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ ë¼ë²¨ì„ ë§Œë“ ë‹¤.\n",
        "        # 3) textì—ì„œ ì²« eos í† í° ì´í›„ ë°˜ë³µë˜ëŠ” eos í† í°ì— ëŒ€í•´ì„œ labelì„ -100ìœ¼ë¡œ ì„¤ì •í•œë‹¤.\n",
        "\n",
        "        \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return labels\n",
        "\n",
        "\n",
        "    def preprocess_inputs(self, img_path, text_prompt):\n",
        "\n",
        "        return preprocess_inputs(\n",
        "            self,\n",
        "            img_path=img_path,\n",
        "            text_prompt=text_prompt,\n",
        "        )\n",
        "\n",
        "    def top_k_sampling(self, logits, top_k, temperature):\n",
        "\n",
        "        return top_k_sampling(\n",
        "            self,\n",
        "            logits=logits,\n",
        "            top_k=top_k,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "    def top_p_sampling(self, logits, top_p, temperature):\n",
        "\n",
        "        return top_p_sampling(\n",
        "            self,\n",
        "            logits=logits,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        embeddings: TensorType[\"b\", \"s\", \"d\"],\n",
        "        max_steps: int = 100,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 0.9,\n",
        "        decode: bool = True,\n",
        "        temperature: float = 0.7,\n",
        "\n",
        "    ):\n",
        "\n",
        "        return generate(\n",
        "            self,\n",
        "            embeddings=embeddings,\n",
        "            max_steps=max_steps,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            decode=decode,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: TensorType[\"b\", \"c\", \"h\", \"w\"] = None,\n",
        "        captions: Optional[TensorType[\"b\", \"seq\"]] = None,\n",
        "        output_hidden_states: bool = True,\n",
        "        image_embeddings: TensorType[\"b\", \"s\", \"d\"] = None,\n",
        "        attention_mask=None,\n",
        "    ) -> ModelOutput:\n",
        "\n",
        "        # í›ˆë ¨ ì‹œ LLMì— ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°ˆ ì„ë² ë”©ì„ ì–»ëŠ”ë‹¤.\n",
        "        image_embeddings, caption_embeddings, input_embeddings = self.make_input_embeddings(images,captions,image_embeddings)\n",
        "\n",
        "        # ì´ë¯¸ì§€ì™€ ìº¡ì…˜ìœ¼ë¡œë¶€í„° ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ ë¼ë²¨ì„ ë§Œë“ ë‹¤.\n",
        "        labels = self.build_labels_for_training(image_embeddings, captions)\n",
        "\n",
        "        # Language Model ì¶”ë¡ \n",
        "        lm_outputs = super().forward(\n",
        "            inputs_embeds=input_embeddings,\n",
        "            labels=labels,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=False,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        return lm_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k82NzVNNSOjh"
      },
      "source": [
        "í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "494mC_TkuEV6"
      },
      "outputs": [],
      "source": [
        "# Dataset parameters\n",
        "train_dataset_dir='./limber/cc3m/train'\n",
        "\n",
        "# Model parameters\n",
        "max_seq_len=2048 # ì–¸ì–´ ëª¨ë¸ì˜ ì…ë ¥ì— ëŒ€í•œ ìµœëŒ€ ê¸¸ì´(í† í° ìˆ˜ë¡œ ì¸¡ì •)\n",
        "encoder_out_dim=3072 # ì´ë¯¸ì§€ ì¸ì½”ë”ì˜ ì¶œë ¥ ì°¨ì› (=h_I)\n",
        "lm_out_dim=2048 # ì–¸ì–´ ëª¨ë¸ì˜ ì…ë ¥ ë° ì¶œë ¥ ì°¨ì› (=e_L)\n",
        "out_seq_len=144 # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì‹œí€€ìŠ¤ ê¸¸ì´ (=k)\n",
        "image_embed_dropout_prob=0.1 # ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ì— ì ìš©í•  dropout probability\n",
        "\n",
        "# Training parameters\n",
        "weight_decay=0.0\n",
        "train_batch_size=16\n",
        "min_lr=0.0\n",
        "lr=8.0e-4\n",
        "image_enc_lr=2e-06\n",
        "gradient_accumulation_steps=8\n",
        "train_steps=100\n",
        "freeze_lm=True\n",
        "freeze_img_encoder=True\n",
        "save='./limber/checkpoints'\n",
        "exp_name='train'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnUBBmr3Xy9-"
      },
      "source": [
        "í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ì™€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-51Hz3WFXt04"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "import  PIL.Image as PilImage\n",
        "from dataclasses import dataclass\n",
        "from typing import Union, Any, Callable\n",
        "from transformers.utils import PaddingStrategy\n",
        "from collections.abc import Mapping\n",
        "from collections import defaultdict\n",
        "\n",
        "# ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤\n",
        "class Lambda(torch.nn.Module):\n",
        "    def __init__(self, fn: Callable):\n",
        "        super().__init__()\n",
        "        assert hasattr(fn, \"__call__\")\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(x)\n",
        "\n",
        "@dataclass\n",
        "class ImageCaptionDataCollator:\n",
        "    tokenizer: GPT2TokenizerFast\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = {}\n",
        "        batch['input_ids']=[f[\"captions\"] for f in features]\n",
        "        batch = self.tokenizer.pad(\n",
        "            batch,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=self.return_tensors,\n",
        "        )\n",
        "        batch['captions']=batch['input_ids']\n",
        "        batch['images'] = torch.stack([i['images'] for i in features]).float()\n",
        "        batch['attention_mask'] = torch.stack([torch.cat( (torch.ones(out_seq_len), a) ) for a in batch['attention_mask']]).float()\n",
        "        del batch['input_ids']\n",
        "        return batch\n",
        "\n",
        "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
        "def add_batch_dim(t):\n",
        "  return t.unsqueeze(0)\n",
        "\n",
        "def get_params_for_weight_decay_optimization(module):\n",
        "    weight_decay_params = {\"params\": []}\n",
        "    no_weight_decay_params = {\"params\": [], \"weight_decay\": 0.0}\n",
        "    blacklist_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "\n",
        "    for module_ in module.modules():\n",
        "        if isinstance(module_, blacklist_modules) or (\n",
        "            weight_decay == 0.0\n",
        "        ):\n",
        "            no_weight_decay_params[\"params\"].extend(\n",
        "                [\n",
        "                    p\n",
        "                    for p in list(module_._parameters.values())\n",
        "                    if (p is not None) and p.requires_grad\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            for n, p in list(module_._parameters.items()):\n",
        "                if p is not None and p.requires_grad:\n",
        "                    if n != \"bias\":\n",
        "                        weight_decay_params[\"params\"].append(p)\n",
        "                    else:\n",
        "                        no_weight_decay_params[\"params\"].append(p)\n",
        "    param_dict = {\n",
        "        pn: p\n",
        "        for pn, p in module.named_parameters()\n",
        "        if p is not None and p.requires_grad\n",
        "    }\n",
        "    assert len(no_weight_decay_params[\"params\"]) + len(\n",
        "        weight_decay_params[\"params\"]\n",
        "    ) == len(\n",
        "        param_dict.keys()\n",
        "    ), \"Number of params in both groups != total number of trainable params\"\n",
        "    if weight_decay == 0.0:\n",
        "        # only return a single param group if no weight decay is being used anyway\n",
        "        return [no_weight_decay_params]\n",
        "    return [weight_decay_params, no_weight_decay_params]\n",
        "\n",
        "def configure_param_groups(model):\n",
        "    if image_enc_lr is not None:\n",
        "        image_enc_params = get_params_for_weight_decay_optimization(\n",
        "            model.image_prefix.encoder\n",
        "        )\n",
        "        for pdict in image_enc_params:\n",
        "            pdict[\"lr\"] = image_enc_lr\n",
        "        image_proj_params = get_params_for_weight_decay_optimization(\n",
        "            model.image_prefix.proj\n",
        "        )\n",
        "\n",
        "        lm_params = get_params_for_weight_decay_optimization(model.transformer)\n",
        "        lm_params +=get_params_for_weight_decay_optimization(model.lm_head)\n",
        "\n",
        "        class_params = []\n",
        "        if hasattr(model, \"class_head\") and model.class_head is not None:\n",
        "            class_params = get_params_for_weight_decay_optimization(\n",
        "                model.class_head\n",
        "            )\n",
        "        all_params = []\n",
        "        for p in image_enc_params + lm_params + image_proj_params + class_params:\n",
        "            if p[\"params\"]:\n",
        "                all_params.append(p)\n",
        "    else:\n",
        "        all_params = get_params_for_weight_decay_optimization(model)\n",
        "\n",
        "    d = defaultdict(dict)\n",
        "    for param_group in all_params:\n",
        "        lr = param_group.get(\"lr\", None)\n",
        "        wd = param_group.get(\"weight_decay\", None)\n",
        "        key = f\"lr_{lr}_wd_{wd}\"\n",
        "        if d[key].get(\"params\") is None:\n",
        "            d[key][\"params\"] = []\n",
        "        d[key][\"params\"].extend(param_group[\"params\"])\n",
        "        if lr is not None:\n",
        "            d[key][\"lr\"] = lr\n",
        "        if wd is not None:\n",
        "            d[key][\"weight_decay\"] = wd\n",
        "    all_params = list(d.values())\n",
        "    n_params = sum([len(d[\"params\"]) for d in all_params])\n",
        "    param_dict = {\n",
        "        pn: p for pn, p in model.named_parameters() if p is not None and p.requires_grad\n",
        "    }\n",
        "    assert n_params == len(\n",
        "        param_dict\n",
        "    ), f\"Some parameters are missing from param groups ({n_params} | {len(param_dict)})\"\n",
        "    return all_params\n",
        "\n",
        "def _prepare_input(data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:\n",
        "    if isinstance(data, Mapping):\n",
        "        return type(data)({k: _prepare_input(v) for k, v in data.items()})\n",
        "    elif isinstance(data, (tuple, list)):\n",
        "        return type(data)(_prepare_input(v) for v in data)\n",
        "    elif isinstance(data, torch.Tensor):\n",
        "        kwargs = dict(device='cuda')\n",
        "        return data.to(**kwargs)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwdafJ59I7XS"
      },
      "source": [
        "### **[Req. 1-1]** ë°ì´í„° ì „ì²˜ë¦¬ : ImageCaptionDataset ë‚´ \\_\\_getitem\\_\\_() ë©”ì†Œë“œ êµ¬í˜„\n",
        "ë³¸ ì‹¤ìŠµì€ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í…ì„œ í˜•íƒœì˜ ì´ë¯¸ì§€ë¡œ, ìº¡ì…˜ì„ í…ì„œ í˜•íƒœì˜ í† í°ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ImageCaptionDataset í´ë˜ìŠ¤ì˜ \\_\\_getitem\\_\\_() ë©”ì†Œë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2PFzffHPkGQ"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_utils import no_init_weights\n",
        "\n",
        "with no_init_weights():\n",
        "    model = LimberGPTNeo.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
        "\n",
        "model.setup_multimodal(tokenizer, max_seq_len, encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, freeze_lm, freeze_img_encoder)\n",
        "\n",
        "tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
        "transforms = model.transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdCug3_jhECV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# dataset\n",
        "CC3M = ImageCaptionDataset(\n",
        "        train_dataset_dir, tokenizer, transforms\n",
        "    )\n",
        "\n",
        "CC3M_dataloader = DataLoader(\n",
        "    CC3M,\n",
        "    batch_size=1,\n",
        "    collate_fn=ImageCaptionDataCollator(tokenizer)\n",
        ")\n",
        "test_inputs = next(iter(CC3M_dataloader))\n",
        "\n",
        "test_images = test_inputs[\"images\"]\n",
        "test_captions = test_inputs[\"captions\"]\n",
        "print(f\"test_images.shape : {test_images.shape}\")\n",
        "print(f\"test_captions.shape : {test_captions.shape}\")\n",
        "print(f\"test_captions : {test_captions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeHyiYGiNY05"
      },
      "source": [
        "### **[Req. 1-2]** ì´ë¯¸ì§€ë¥¼ ì…ë ¥ ë°›ì•„ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ImagePrefix í´ë˜ìŠ¤ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkkpqA90NYaa"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "test_images = test_images.to(device)\n",
        "test_captions = test_captions.to(device)\n",
        "\n",
        "with autocast(dtype=torch.float16):\n",
        "  test_image_representations=model.image_prefix.encoder(test_images)\n",
        "  test_image_prompts=model.image_prefix(test_images)\n",
        "\n",
        "print(f\"test_image_representations.shape : {test_image_representations.shape}\")\n",
        "print(f\"test_image_prompts.shape : {test_image_prompts.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLy4-gxiUhmg"
      },
      "source": [
        "\n",
        "\n",
        "### **[Req. 1-3]** LimberGPTJ ë‚´ setmultimodal í•¨ìˆ˜ êµ¬í˜„\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUaQtfI1V7op"
      },
      "source": [
        "### **[Req. 1-4]** LimberGPTJ ë‚´ make_input_embeddings í•¨ìˆ˜ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_VpMwznU-MM"
      },
      "outputs": [],
      "source": [
        "with autocast(dtype=torch.float16):\n",
        "  test_image_prompts, test_caption_embeddings, test_input_embeddings =model.make_input_embeddings(test_images, test_captions)\n",
        "\n",
        "print(f\"test_image_prompts.shape : {test_image_prompts.shape}\")\n",
        "print(f\"test_caption_embeddings.shape : {test_caption_embeddings.shape}\")\n",
        "print(f\"test_input_embeddings.shape : {test_input_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1ZwA2_EV_sn"
      },
      "source": [
        "### **[Req. 1-5]** LimberGPTJ ë‚´ build_labels_for_training í•¨ìˆ˜ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9zIbXohWSOB"
      },
      "outputs": [],
      "source": [
        "with autocast(dtype=torch.float16):\n",
        "  test_labels = model.build_labels_for_training(test_image_prompts,test_captions)\n",
        "\n",
        "print(f\"test_labels[0] : {test_labels[0]}\")\n",
        "print(f\"test_labels.shape : {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1R0hJIxPezc"
      },
      "source": [
        "## 3. LimberGPTJ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-gaMt0If16I"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from torch.cuda.amp import autocast\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "save_dir = os.path.join(save, exp_name)\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Dataset ì •ì˜\n",
        "train_dataset = ImageCaptionDataset(\n",
        "        train_dataset_dir, tokenizer, transforms\n",
        "    )\n",
        "\n",
        "# Dataloader ì •ì˜\n",
        "generator = torch.Generator()\n",
        "seed = 2024\n",
        "generator.manual_seed(seed)\n",
        "\n",
        "train_sampler = RandomSampler(train_dataset, generator=generator)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    sampler=train_sampler,\n",
        "    collate_fn=ImageCaptionDataCollator(tokenizer),\n",
        "    drop_last=True,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Optimizer ì •ì˜\n",
        "trainable_parameters = configure_param_groups(model)\n",
        "optimizer = AdamW(\n",
        "        trainable_parameters,\n",
        "        lr,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "scheduler = WarmupDecayLR(optimizer, lr_decay_iters)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "print(\"Start Training!\")\n",
        "model.train()\n",
        "model.gradient_checkpointing_enable()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "max_steps = train_steps\n",
        "total_train_batch_size = train_batch_size * gradient_accumulation_steps\n",
        "steps_in_epoch = len(train_dataloader)\n",
        "num_update_steps_per_epoch = steps_in_epoch // gradient_accumulation_steps\n",
        "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
        "num_train_epochs = max_steps // num_update_steps_per_epoch + int(\n",
        "    max_steps % num_update_steps_per_epoch > 0\n",
        ")\n",
        "num_train_samples = max_steps * total_train_batch_size\n",
        "\n",
        "print(\"***** Running training *****\")\n",
        "print(f\"  Num examples = {len(train_dataloader.dataset)}\")\n",
        "print(f\"  Num Epochs = {num_train_epochs}\")\n",
        "print(f\"  Total train batch size = {total_train_batch_size}\")\n",
        "print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
        "print(f\"  Total optimization steps = {max_steps}\")\n",
        "\n",
        "global_step=0\n",
        "tr_loss = torch.tensor(0.0).to(device)\n",
        "model.zero_grad()\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "  for step, inputs in enumerate(train_dataloader):\n",
        "    inputs = _prepare_input(inputs)\n",
        "    with autocast(dtype=torch.float16):\n",
        "        outputs = model(**inputs)\n",
        "        # Loss ì •ì˜\n",
        "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "    loss = loss / gradient_accumulation_steps\n",
        "    # Backpropagation\n",
        "    scaler.scale(loss).backward()\n",
        "    tr_loss_step = loss.detach()\n",
        "    tr_loss += tr_loss_step\n",
        "\n",
        "    if (step + 1) % gradient_accumulation_steps == 0:\n",
        "      scaler.unscale_(optimizer)\n",
        "      nn.utils.clip_grad_norm_(\n",
        "          model.parameters(),\n",
        "          1.0,\n",
        "      )\n",
        "      # Optimizer step\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      model.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "      # í˜„ì¬ step numberì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜(weights)ì™€ í¸í–¥(bias)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "      if global_step % 10 == 0 :\n",
        "\n",
        "        # ëª¨ë¸ì˜ state_dictì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "        state_dict = model.state_dict()\n",
        "        # íŠ¹ì • layer(projection layer)ì˜ ê°€ì¤‘ì¹˜(weights)ì™€ í¸í–¥(bias)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "        selected_layer_params = {key: value for key, value in state_dict.items() if 'image_prefix.proj' in key}\n",
        "        save_path = f\"{save_dir}/step_{global_step}.ckpt\"\n",
        "        torch.save(selected_layer_params, save_path)\n",
        "        print(\"Model saved in \", save_path)\n",
        "\n",
        "      tr_loss_scaler = round(tr_loss.mean().item(), 4)\n",
        "      tr_loss -= tr_loss\n",
        "      print(f\"'loss': {tr_loss_scaler}, 'step': {global_step}\")\n",
        "\n",
        "print(f\"\\n\\nTraining completed in {time.time() - start_time:.5f} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpGtxThJTK1v"
      },
      "source": [
        "# 4. ì‹œê°-ì–¸ì–´ ë©€í‹°ëª¨ë‹¬ LLMì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±\n",
        "\n",
        "LimberGPTJ í´ë˜ìŠ¤ì˜ í•¨ìˆ˜ë“¤ì„ í•˜ë‚˜ì”© êµ¬í˜„í•˜ë©´ì„œ Req. 1-6ë¶€í„° Req. 1-8ì„ í’€ì–´ë´…ë‹ˆë‹¤. êµ¬í˜„ì— í•„ìš”í•œ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- LimberGPTJ í´ë˜ìŠ¤  \n",
        "  - **Req. 1-6**: LimberGPTJ ë‚´ preprocess_inputs í•¨ìˆ˜ êµ¬í˜„\n",
        "  - **Req. 1-7**: LimberGPTJ ë‚´ top_k_sampling í•¨ìˆ˜ êµ¬í˜„\n",
        "  - **Req. 1-8**: LimberGPTJ ë‚´ top_p_sampling í•¨ìˆ˜ êµ¬í˜„  \n",
        "ì•„ë˜ ì…€ë¶€í„° í•˜ë‚˜ì”© ë”°ë¼ê°€ë©´ì„œ Req. 1-6 ì™€ Req. 1-8 ì—ì„œ êµ¬í˜„ì´ í•„ìš”í•œ ë¶€ë¶„ì„ ì½ê³ , ë‹¤ì‹œ LimberGPTJ í´ë˜ìŠ¤ì˜ í•¨ìˆ˜ íŒŒíŠ¸ë¡œ ëŒì•„ì™€ì„œ í•´ë‹¹ \"\"\"Write your code\"\"\" ë¶€ë¶„ì„ êµ¬í˜„í•´ë´…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6epeGG7oN5T0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torchtyping import TensorType\n",
        "from typing import Union, List\n",
        "\n",
        "############################################################################\n",
        "# Req 1-6: LimberGPTJ ë‚´ preprocess_inputs í•¨ìˆ˜ êµ¬í˜„                          #\n",
        "############################################################################\n",
        "def preprocess_inputs(model, img_path, text_prompt):\n",
        "    \"\"\"\n",
        "    :param img_path: ì´ë¯¸ì§€ ê²½ë¡œ\n",
        "    :param text_prompt: ê³ ì •ëœ initial text prompt\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: ì´ë¯¸ì§€ì™€ ê³ ì •ëœ initial text promptë¥¼ ì–¸ì–´ ëª¨ë¸ì˜ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°ˆ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜     #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) img_pathë¥¼ í…ì„œ í˜•íƒœì˜ ì´ë¯¸ì§€ë¡œ, ê³ ì •ëœ initial text promptë¥¼ í…ì„œ í˜•íƒœì˜ í† í°ìœ¼ë¡œ ë³€í™˜\n",
        "    # 2) ì´ë¯¸ì§€ë¡œë¶€í„° ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ì–»ê³ , í† í°ìœ¼ë¡œë¶€í„° text promptì˜ ì„ë² ë”©ì„ ì–»ì–´ ì¶”ë¡  ì‹œ ì–¸ì–´ ëª¨ë¸ì—\n",
        "    #    ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°ˆ ì„ë² ë”© ì–»ê¸°\n",
        "\n",
        "    \"\"\"Write your code\"\"\"\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    return input_embedding\n",
        "\n",
        "############################################################################\n",
        "# Req 1-7: LimberGPTJ ë‚´ top_k_sampling í•¨ìˆ˜ êµ¬í˜„                             #\n",
        "############################################################################\n",
        "def top_k_sampling(model,logits, top_k, temperature):\n",
        "    \"\"\"\n",
        "    :param logits: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ í† í°ì˜ probability (vocabularyì— ìˆëŠ” 50258 í† í°ì˜ logit ê°’)\n",
        "    :param top_k: ë‹¤ìŒ í† í° ì„ íƒì‹œ, ìƒìœ„ kê°œì˜ í† í°ì„ í›„ë³´ë¡œ ê³ ë ¤\n",
        "    :param temperature: temperature scaling ê°’ (ì˜ˆì¸¡í•œ ë‹¤ìŒ í† í°ë“¤ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë³€í˜•ì‹œí‚´.\n",
        "                        0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ í™•ë¥  ë¶„í¬ê°€ ë‚ ì¹´ë¡œì›Œì§€ë©°, 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ í™•ë¥  ë¶„í¬ê°€ í‰í‰í•´ì§.)\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: ì–¸ì–´ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ ë‹¨ì–´ì— ëŒ€í•œ logits(ëª¨ë“  í† í°ì˜ logit ê°’)ë¡œë¶€í„°              #\n",
        "    # Top-K sampling ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì„ ìƒ˜í”Œë§                                    #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) torch.topk í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ logitsë¡œë¶€í„° ìƒìœ„ kê°œ í† í°ì˜ logitê³¼ indexë¥¼ ì¶”ì¶œ\n",
        "    # 2) torch.full_like í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ -infinite ê°’ìœ¼ë¡œ ì±„ì›Œì§„ logits í¬ê¸°ì˜ í…ì„œ ìƒì„±\n",
        "    # 3) 1ì—ì„œ êµ¬í•œ ìƒìœ„ kê°œ í† í°ì˜ logitì„ 2ì—ì„œ ìƒì„±í•œ í…ì„œì˜ í•´ë‹¹ indexì— assign\n",
        "    # 4) torch.nn.functionalì˜ softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì˜ probability ê³„ì‚°\n",
        "    # 5) torch.multinomial í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ probabilityë¡œë¶€í„° í•œ ê°œ í† í°ì„ ìƒ˜í”Œë§ ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ\n",
        "\n",
        "    \"\"\"Write your code\"\"\"\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    return next_token\n",
        "\n",
        "############################################################################\n",
        "# Req 1-8: LimberGPTJ ë‚´ top_p_sampling í•¨ìˆ˜ êµ¬í˜„                             #\n",
        "############################################################################\n",
        "def top_p_sampling(model,logits, top_p, temperature):\n",
        "    \"\"\"\n",
        "    :param logits: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ í† í°ì˜ probability (vocabularyì— ìˆëŠ” 50258 í† í°ì˜ logit ê°’)\n",
        "    :param top_p: ë‹¤ìŒ í† í° ì„ íƒì‹œ, ëˆ„ì  í™•ë¥ ì´ p ì´ìƒì´ ë˜ëŠ” ìµœì†Œí•œì˜ í† í° ì§‘í•©ì„ í›„ë³´ë¡œ ê³ ë ¤\n",
        "    :param temperature: temperature scaling ê°’ (ì˜ˆì¸¡í•œ ë‹¤ìŒ í† í°ë“¤ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë³€í˜•ì‹œí‚´.\n",
        "                        0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ í™•ë¥  ë¶„í¬ê°€ ë‚ ì¹´ë¡œì›Œì§€ë©°, 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ í™•ë¥  ë¶„í¬ê°€ í‰í‰í•´ì§.)\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: ì–¸ì–´ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ ë‹¨ì–´ì— ëŒ€í•œ logits(ëª¨ë“  í† í°ì˜ logit ê°’)ë¡œë¶€í„°              #\n",
        "    # Top-P sampling ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì„ ìƒ˜í”Œë§                                    #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) torch.sort í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ logitsë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "    # 2) torch.cumsum í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ logitsì˜ ëˆ„ì  ë¶„í¬ ê³„ì‚°\n",
        "    # 3) ëˆ„ì  ë¶„í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ í† í°ì´ í›„ë³´ì—ì„œ ì œì™¸í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‹´ì€ í…ì„œ ìƒì„±\n",
        "    #    i. ëˆ„ì  í™•ë¥ ì´ p ì´ìƒì´ ë˜ëŠ” indexì— True ê°’ í• ë‹¹\n",
        "    #    ii. ëˆ„ì  í™•ë¥ ì´ p ì´ìƒì´ ë˜ëŠ” ìµœì†Œí•œì˜ í† í° ì§‘í•©ì´ë¯€ë¡œ, ìœ„ì—ì„œ í• ë‹¹í•œ ê°’ë“¤ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ì¹¸ ì”© ì´ë™\n",
        "    #    iii. í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 0ë²ˆì§¸ indexëŠ” í•­ìƒ False ê°’ í• ë‹¹\n",
        "    # 4) í›„ë³´ì—ì„œ ì œì™¸ë˜ëŠ” í† í°ì˜ logitì€ -infiniteë¡œ í• ë‹¹\n",
        "    # 5) torch.nn.functionalì˜ softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì˜ probability ê³„ì‚°\n",
        "    # 6) torch.multinomial í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ probabilityë¡œë¶€í„° í•œ ê°œ í† í°ì„ ìƒ˜í”Œë§\n",
        "\n",
        "    \"\"\"Write your code\"\"\"\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    return next_token\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model: \"GPTNeo\",\n",
        "    embeddings: TensorType[\"b\", \"s\", \"d\"],\n",
        "    max_steps: int = 100,\n",
        "    top_k: int = 0,\n",
        "    top_p: float = 0.9,\n",
        "    decode: bool = True,\n",
        "    temperature: float = 0.7,\n",
        "\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    embeddingì— ëŒ€í•œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "    :param model: ìº¡ì…˜ ìƒì„±ì— ì“°ì¼ ëª¨ë¸\n",
        "    :param embeddings: ìº¡ì…˜ì„ ìƒì„±í•  ì„ë² ë”©\n",
        "    :param max_steps: ìº¡ì…˜ì„ ìƒì„±í•  ë•Œ ìµœëŒ€ step ìˆ˜\n",
        "    :param top_k: Top k ìƒ˜í”Œë§ì— ì‚¬ìš©í•  ê°’, 0ì´ë©´ ìƒ˜í”Œë§ì´ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ\n",
        "    :param top_p: Top p ìƒ˜í”Œë§ì— ì‚¬ìš©í•  ê°’, 0ì´ë©´ ìƒ˜í”Œë§ì´ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ\n",
        "    :param decode: í† í°ì„ decodeí•˜ì—¬ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í• ì§€ ë˜ëŠ” í† í°ì„ ë°˜í™˜í• ì§€ ì—¬ë¶€\n",
        "    :param temperature: temperature scaling ê°’ (ì˜ˆì¸¡í•œ ë‹¤ìŒ í† í°ë“¤ì˜ í™•ë¥  ë¶„í¬ë¥¼ ë³€í˜•ì‹œí‚´.\n",
        "                        0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ í™•ë¥  ë¶„í¬ê°€ ë‚ ì¹´ë¡œì›Œì§€ë©°, 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ í™•ë¥  ë¶„í¬ê°€ í‰í‰í•´ì§.)\n",
        "    \"\"\"\n",
        "\n",
        "    eos_token = model.eos_token\n",
        "    b, s, _ = embeddings.shape\n",
        "    past_key_values = None\n",
        "\n",
        "    # ì´ë¯¸ì§€ í† í°ìœ¼ë¡œ outputì„ ì´ˆê¸°í™”\n",
        "    out = torch.zeros((b, s), dtype=torch.long).to(model.device) + model.image_token\n",
        "\n",
        "    # sampling\n",
        "    for i in range(max_steps):\n",
        "        if i == 0:\n",
        "            # ì´ˆê¸° input\n",
        "            outputs = super(type(model),model).forward(\n",
        "                inputs_embeds=embeddings,\n",
        "                use_cache=True,\n",
        "                past_key_values=past_key_values,\n",
        "            )\n",
        "        else:\n",
        "            # ê³¼ê±° keyì™€ valueë¥¼ ìºì‹±í•˜ì—¬ ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš©\n",
        "            outputs = super(type(model),model).forward(\n",
        "                input_ids=out[:, -1:], use_cache=True, past_key_values=past_key_values\n",
        "            )\n",
        "\n",
        "        logits = outputs.logits[:, -1, :].float()\n",
        "        past_key_values = outputs.past_key_values\n",
        "\n",
        "        if top_k > 0:\n",
        "          # Top K sampling\n",
        "          next_token = model.top_k_sampling(logits,top_k,temperature)\n",
        "        if top_p > 0:\n",
        "          # Nucleus sampling\n",
        "          next_token = model.top_p_sampling(logits,top_p,temperature)\n",
        "\n",
        "        out = torch.cat((out, next_token), dim=-1)\n",
        "\n",
        "        if eos_token is not None and (next_token == eos_token).all():\n",
        "            break\n",
        "\n",
        "    if decode:\n",
        "        captions = []\n",
        "        for b in out:\n",
        "          eos_index = (b == eos_token).nonzero()\n",
        "          if eos_index.any():\n",
        "            b[eos_index[0] :] = eos_token\n",
        "          b = b.tolist()\n",
        "          b=[i for i in b if (not i == model.image_token) and (not i == eos_token)]\n",
        "          # í† í°ì„ decodeí•˜ì—¬ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜\n",
        "          caption = model.tokenizer.decode(b)\n",
        "          captions.append(caption)\n",
        "        out = captions\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76j0B31vt6nT"
      },
      "source": [
        "**Test Image:**  \n",
        "![test_image.png](https://ak2.picdn.net/shutterstock/videos/18488392/thumb/9.jpg)  \n",
        "(ê·¸ë¦¼ ì¶œì²˜: https://ak2.picdn.net/shutterstock/videos/18488392/thumb/9.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsVBixc6lzpm"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# ì œê³µëœ ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "limber_proj_path = \"./limber/checkpoints/gpt_neo_pretrained_weights.ckpt\"\n",
        "proj_ckpt = torch.load(limber_proj_path)\n",
        "proj_ckpt = {key.split(\"image_prefix.proj.\")[1]: value for key, value in proj_ckpt.items()}\n",
        "model.image_prefix.proj.load_state_dict(proj_ckpt)\n",
        "model = model.cuda().half()\n",
        "\n",
        "start_time = time.time()\n",
        "img_path = \"./limber/cc3m/image_captioning.jpg\"\n",
        "model.eval()\n",
        "print(\"Start inference...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FzYpYoA7-MS"
      },
      "source": [
        "### **[Req. 1-6]** LimberGPTJ ë‚´ preprocess_inputs í•¨ìˆ˜ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lZiXfza8Km3"
      },
      "outputs": [],
      "source": [
        "embeddings = model.preprocess_inputs(img_path,'A picture of')\n",
        "print(f\"embeddings.shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38kTGGwc8whp"
      },
      "source": [
        "### **[Req. 1-7]** LimberGPTJ ë‚´ top_k_sampling í•¨ìˆ˜ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1i_6QKu8N50"
      },
      "outputs": [],
      "source": [
        "caption = model.generate(embeddings=embeddings,top_k=3,temperature=0.7)\n",
        "print(\"\\nOutput: \")\n",
        "print(caption[0].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrIcyObX8zti"
      },
      "source": [
        "### **[Req. 1-8]** LimberGPTJ ë‚´ top_p_sampling í•¨ìˆ˜ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS2c5KuI8sFy"
      },
      "outputs": [],
      "source": [
        "caption = model.generate(embeddings=embeddings,top_p=0.9,temperature=0.7)\n",
        "print(\"\\nOutput: \")\n",
        "print(caption[0].strip())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
