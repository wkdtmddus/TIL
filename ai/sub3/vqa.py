# -*- coding: utf-8 -*-
"""vqa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kp1U-ag2pU0WVT4q0OmNRADgXJvP1WSK

[환경 세팅]

https://drive.google.com/file/d/1NgBmdOVjhdRXYNPtgF1VnyfuhhxhNzEn/view?usp=sharing   
다음 링크에서 limber.zip 파일을 다운로드 한 후, 압축을 해제하고 해당 limber 폴더를 제공된 project/sub3/ 폴더 안에 넣어줍니다.  

디렉토리 구조는 다음과 같습니다.
- 디렉토리 구조  
      |-- project/  
      |   |-- sub1/  
      |   |-- sub2/   
      |   |-- sub3/
      |   |   |-- limber/  
      |   |   |-- limber.ipynb   
      |   |   |-- vqa.ipynb  

필요한 pakage들을 설치합니다.  

      pip install transformers  
      pip install https://github.com/openai/CLIP/archive/master.zip  
      pip install torchtyping  
      pip install einops  
      pip install pyhelpers

현재 작업중인 디렉토리를 project/sub3 디렉토리로 설정합니다.

"""

import torch
import numpy as np
import random
import warnings
warnings.filterwarnings(action='ignore')
from transformers import GPT2TokenizerFast, GPTJForCausalLM
from typing import Optional, Union, Any, Callable
from torchtyping import TensorType
from transformers.file_utils import ModelOutput
from torchvision import transforms as T
from PIL import Image
from functools import partial
from typing import Union
import torch.nn as nn
import clip
from einops import rearrange
from transformers.modeling_utils import no_init_weights
import torch.nn.functional as F
from collections.abc import Mapping

random_seed = 1111
torch.manual_seed(random_seed)
torch.cuda.manual_seed(random_seed)
torch.cuda.manual_seed_all(random_seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(random_seed)
random.seed(random_seed)

# transformers 라이브러리에서 제공하는 pretrained tokenizer를 로드합니다.
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
# pad token id를 eos token id로 지정합니다.
tokenizer.pad_token_id = tokenizer.eos_token_id
# 모델이 padding을 오른쪽에 적용하도록 합니다.
tokenizer.padding_side = "right"
# 모델의 입력에 대한 최대 길이(토큰 수로 측정)를 지정합니다.
tokenizer.model_max_length = 2048
# '<|image|>' 문자열을 클래스 토큰으로 설정하고 tokenizer에 추가합니다.
tokenizer.add_special_tokens(
            {'cls_token': '<|image|>'}
        )

class ImagePrefix(nn.Module):

    """
    이미지 배치를 입력으로 받아 언어 모델의 word embedding과 동일한 h_I 차원의 소프트 프롬프트를 반환합니다.

    :param encoder_out_dim: 이미지 인코더의 출력 차원 (=h_I)
    :param lm_out_dim: 언어 모델의 입력 및 출력 차원 (=e_L)
    :param out_seq_len: 소프트 프롬프트의 시퀀스 길이 (=k)
    :param image_embed_dropout_prob: 소프트 프롬프트에 적용할 dropout probability
    :param device: 모델을 실행 할 장치
    """

    def __init__(
        self,
        encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, device=None,
    ):
        super().__init__()

        # pretrained CLIP image encoder 가져오기
        self.encoder = clip.load("RN50x16", device=device)[0].visual
        self.encoder.attnpool = Lambda(
            partial(rearrange, pattern="b d h w -> b (h w) d")
        )
        # 이미지 인코더의 출력 차원 (=h_I)
        self.encoder_out_dim = encoder_out_dim

        # 언어 모델의 입력 및 출력 차원 (=e_L)
        self.lm_out_dim = lm_out_dim

        # 소프트 프롬프트의 시퀀스 길이 (=k)
        self.out_seq_len = out_seq_len

        # 소프트 프롬프트의 차원
        proj_out_dim = self.lm_out_dim

        # Projection layer 정의
        self.proj = nn.Linear(self.encoder_out_dim, proj_out_dim)

        self.dropout = nn.Dropout(image_embed_dropout_prob)

    def forward(
        self, x: TensorType["b", "c", "h", "w"]
    ) -> TensorType["b", "seq", "out_dim"]:

        image_feats = self.encoder(x)
        soft_prompts = self.proj(image_feats)

        # 소프트 프롬프트에 dropout 적용
        soft_prompts = self.dropout(soft_prompts)

        return soft_prompts

class LimberGPTJ(GPTJForCausalLM):

    def setup_multimodal(self, tokenizer, max_seq_len, encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, freeze_lm, freeze_img_encoder):

        """
        :param tokenizer: tokenizer
        :param max_seq_len: 언어 모델의 입력에 대한 최대 길이(토큰 수로 측정)
        :param encoder_out_dim: 이미지 인코더의 출력 차원 (=h_I)
        :param lm_out_dim: 언어 모델의 입력 및 출력 차원 (=e_L)
        :param out_seq_len: 소프트 프롬프트의 시퀀스 길이 (=k)
        :param image_embed_dropout_prob: 소프트 프롬프트에 적용할 dropout probability
        :param freeze_lm: 언어 모델을 freeze할지 여부
        :param freeze_img_encoder: 이미지 인코더를 freeze할지 여부
        """

        self.seq_len = max_seq_len
        self.tokenizer = tokenizer

        self.image_token = self.tokenizer.cls_token_id
        self.eos_token = self.tokenizer.eos_token_id
        self.resize_token_embeddings(len(self.tokenizer))

        self.word_embedding = self.transformer.wte

        # 언어 모델 freeze
        if freeze_lm:
            for name, param in self.named_parameters():
              param.requires_grad = False

        # ImagePrefix 클래스의 인스턴스 생성
        self.image_prefix = ImagePrefix(
            encoder_out_dim = encoder_out_dim,
            lm_out_dim=lm_out_dim,
            out_seq_len=out_seq_len,
            image_embed_dropout_prob=image_embed_dropout_prob,
        )

        # 이미지 프롬프트의 시퀀스 길이 지정
        self.image_prompt_seq_len = self.image_prefix.out_seq_len
        inp_rez = self.image_prefix.encoder.input_resolution

        self.transforms = T.Compose(
        [
            T.Resize(inp_rez, interpolation=T.InterpolationMode.BICUBIC),
            T.CenterCrop(inp_rez),
            lambda image: image.convert("RGB"),
            T.ToTensor(),
            add_batch_dim,
            T.Normalize(
                (0.48145466, 0.4578275, 0.40821073),
                (0.26862954, 0.26130258, 0.27577711),
            ),
        ]
        )
        # 이미지 인코더 freeze
        if freeze_img_encoder:
            for param in self.image_prefix.encoder.parameters():
                param.requires_grad = False

    def make_input_embeddings(self,
        images: TensorType["b", "c", "h", "w"] = None,
        captions: Optional[TensorType["b", "seq"]] = None,
        image_embeddings: TensorType["b", "s", "d"] = None,):

        """
        :param images: 이미지
        :param captions: 캡션
        :param image_embeddings: 이미지 프롬프트 (inference 시 사용)
        """

        if image_embeddings is None:
            image_embeddings = self.image_prefix(images)

        caption_embeddings = self.word_embedding(captions)

        input_embeddings = torch.cat((image_embeddings, caption_embeddings,), dim=1)

        return image_embeddings, caption_embeddings, input_embeddings

    def build_labels_for_training(self,
        image_embeddings: TensorType["b", "s", "d"] = None,
        captions: Optional[TensorType["b", "seq"]] = None,):

        """
        :param image_embeddings: 이미지 프롬프트
        :param captions: 캡션
        """

        image_embeddings_shape = image_embeddings.shape[:2]
        embedding_tokens = torch.zeros(image_embeddings_shape, dtype=torch.int64).to(self.device) - 100

        labels = torch.cat((embedding_tokens, captions),dim=1)

        for label in labels:
          for k, token in enumerate(label):
              if token == self.eos_token:
                  label[k + 1 :] = -100
                  break

        return labels

    def preprocess_inputs(self, img_path, text_prompt):
        """
        :param img_path: 이미지 경로
        :param text_prompt: 고정된 initial text prompt
        """

        img = Image.open(img_path).convert('RGB')
        img_tensor = model.transforms(img).squeeze()
        token_tensor = model.tokenizer.encode(text_prompt, return_tensors="pt")

        img_tensor = img_tensor.unsqueeze(0).to(model.device).half()
        token_tensor = token_tensor.to(model.device)

        image_prompt = model.image_prefix(img_tensor)
        text_embedding = model.word_embedding(token_tensor)
        input_embedding = torch.cat((image_prompt, text_embedding), dim=1)

        return input_embedding

    def top_k_sampling(self, logits, top_k, temperature):
        """
        :param logits: 모델이 예측한 다음 토큰의 probability (vocabulary에 있는 50258 토큰의 logit 값)
        :param top_k: 다음 토큰 선택시, 상위 k개의 토큰을 후보로 고려
        :param temperature: temperature scaling 값 (예측한 다음 토큰들의 확률 분포를 변형시킴.
                        0에 가까울 수록 확률 분포가 날카로워지며, 1에 가까울 수록 확률 분포가 평평해짐.)
        """

        val, ind = torch.topk(logits, top_k)
        probs = torch.full_like(logits, float("-inf"))
        probs.scatter_(1, ind, val)
        logits = probs
        probs = F.softmax(logits/temperature, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        return next_token

    def top_p_sampling(self, logits, top_p, temperature):
        """
        :param logits: 모델이 예측한 다음 토큰의 probability (vocabulary에 있는 50258 토큰의 logit 값)
        :param top_p: 다음 토큰 선택시, 누적 확률이 p 이상이 되는 최소한의 토큰 집합을 후보로 고려
        :param temperature: temperature scaling 값 (예측한 다음 토큰들의 확률 분포를 변형시킴.
                        0에 가까울 수록 확률 분포가 날카로워지며, 1에 가까울 수록 확률 분포가 평평해짐.)
        """

        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        sorted_indices_to_remove = cum_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        sorted_logits[sorted_indices_to_remove] = float("-inf")
        logits = sorted_logits.scatter(1, sorted_indices, sorted_logits)
        probs = F.softmax(logits/temperature, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        return next_token

    @torch.no_grad()
    def generate(
            self,
            embeddings: TensorType["b", "s", "d"],
            max_steps: int = 100,
            top_k: int = 0,
            top_p: float = 0.9,
            decode: bool = True,
            temperature: float = 0.7,
    ):

        """
        embedding에 대한 캡션을 생성합니다.

        :param model: 캡션 생성에 쓰일 모델
        :param embeddings: 캡션을 생성할 임베딩
        :param max_steps: 캡션을 생성할 때 최대 step 수
        :param top_k: Top k 샘플링에 사용할 값, 0이면 샘플링이 사용되지 않음
        :param top_p: Top p 샘플링에 사용할 값, 0이면 샘플링이 사용되지 않음
        :param decode: 토큰을 decode하여 텍스트 형태로 반환할지 또는 토큰을 반환할지 여부
        :param temperature: temperature scaling 값 (예측한 다음 토큰들의 확률 분포를 변형시킴.
                        0에 가까울 수록 확률 분포가 날카로워지며, 1에 가까울 수록 확률 분포가 평평해짐.)
        """

        eos_token = model.eos_token
        b, s, _ = embeddings.shape
        past_key_values = None

        # 이미지 토큰으로 output을 초기화
        out = torch.zeros((b, s), dtype=torch.long).to(model.device) + model.image_token

        # sampling
        for i in range(max_steps):
            if i == 0:
                # 초기 input
                outputs = super(type(model), model).forward(
                    inputs_embeds=embeddings,
                    use_cache=True,
                    past_key_values=past_key_values,
                )
            else:
                # 과거 key와 value를 캐싱하여 마지막 토큰만 사용
                outputs = super(type(model), model).forward(
                    input_ids=out[:, -1:], use_cache=True, past_key_values=past_key_values
                )

            logits = outputs.logits[:, -1, :].float()
            past_key_values = outputs.past_key_values

            if top_k > 0:
                # Top K sampling
                next_token = model.top_k_sampling(logits, top_k, temperature)
            if top_p > 0:
                # Nucleus sampling
                next_token = model.top_p_sampling(logits, top_p, temperature)

            out = torch.cat((out, next_token), dim=-1)

            if eos_token is not None and (next_token == eos_token).all():
                break

        if decode:
            captions = []
            for b in out:
                eos_index = (b == eos_token).nonzero()
                if eos_index.any():
                    b[eos_index[0]:] = eos_token
                b = b.tolist()
                b = [i for i in b if (not i == model.image_token) and (not i == eos_token)]
                # 토큰을 decode하여 텍스트 형태로 변환
                caption = model.tokenizer.decode(b)
                captions.append(caption)
            out = captions

        return out

    def forward(
        self,
        images: TensorType["b", "c", "h", "w"] = None,
        captions: Optional[TensorType["b", "seq"]] = None,
        output_hidden_states: bool = True,
        image_embeddings: TensorType["b", "s", "d"] = None,
        attention_mask=None,
    ) -> ModelOutput:

        assert captions is not None, "Must provide captions in training"
        assert any([i is not None for i in [images, image_embeddings]]) and not all(
            [i is not None for i in [images, image_embeddings]]
        ), "Pass in either images, or input embeddings, not both."

        # 훈련 시 LLM에 인풋으로 들어갈 임베딩을 얻는다.
        image_embeddings, caption_embeddings, input_embeddings = self.make_input_embeddings(images,captions,image_embeddings)

        # 이미지와 캡션으로부터 언어 모델을 훈련하기 위한 라벨을 만든다.
        labels = self.build_labels_for_training(image_embeddings, captions)

        # Language Model 추론
        lm_outputs = super().forward(
            inputs_embeds=input_embeddings,
            labels=labels,
            output_hidden_states=output_hidden_states,
            attention_mask=attention_mask,
            use_cache=False,
            return_dict=True,
        )
        return lm_outputs

# Model parameters
max_seq_len=2048 # 언어 모델의 입력에 대한 최대 길이(토큰 수로 측정)
encoder_out_dim=3072 # 이미지 인코더의 출력 차원 (=h_I)
lm_out_dim=4096 # 언어 모델의 입력 및 출력 차원 (=e_L)
out_seq_len=144 # 이미지 프롬프트 시퀀스 길이 (=k)
image_embed_dropout_prob=0.1 # 소프트 프롬프트에 적용할 dropout probability
freeze_lm=True
freeze_img_encoder=True

# 유틸리티 클래스
class Lambda(torch.nn.Module):
    def __init__(self, fn: Callable):
        super().__init__()
        assert hasattr(fn, "__call__")
        self.fn = fn

    def forward(self, x):
        return self.fn(x)

# 유틸리티 함수
def add_batch_dim(t):
  return t.unsqueeze(0)

def _prepare_input(data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:
    if isinstance(data, Mapping):
        return type(data)({k: _prepare_input(v) for k, v in data.items()})
    elif isinstance(data, (tuple, list)):
        return type(data)(_prepare_input(v) for v in data)
    elif isinstance(data, torch.Tensor):
        kwargs = dict(device='cuda')
        return data.to(**kwargs)
    return data

with no_init_weights():
    model = LimberGPTJ.from_pretrained('EleutherAI/gpt-j-6B')

model.setup_multimodal(tokenizer, max_seq_len, encoder_out_dim, lm_out_dim, out_seq_len, image_embed_dropout_prob, freeze_lm, freeze_img_encoder)

tokenizer.deprecation_warnings["Asking-to-pad-a-fast-tokenizer"] = True
transforms = model.transforms

limber_proj_path = "./limber/checkpoints/gpt_j_pretrained_weights.ckpt"
#limber_proj_path = "./limber/gpt_j_pretrained_weights.ckpt"
proj_ckpt = torch.load(limber_proj_path)
model.image_prefix.proj.load_state_dict(proj_ckpt)
model = model.cuda().half()

img_path = "./limber/cc3m/vqa.jpg"
model.eval()

text_prompt = 'Q:What is the person doing? A:'
embeddings = model.preprocess_inputs(img_path,text_prompt)

caption = model.generate(embeddings=embeddings,top_k=3, temperature=1)
print(f"{text_prompt}{caption[0].strip()}")

caption = model.generate(embeddings=embeddings,top_p=0.9, temperature=1)
print(f"{text_prompt}{caption[0].strip()}")